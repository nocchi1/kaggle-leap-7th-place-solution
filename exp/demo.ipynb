{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import seed_everything, get_logger, get_config, TimeUtil\n",
    "from src.utils.competition_utils import clipping_input\n",
    "from src.data import DataProvider, FeatureEngineering, Preprocessor, HFPreprocessor\n",
    "from src.train import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = '146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-10 16:44:05\u001b[0m | \u001b[1mINFO ] exp: 146 | run_mode=hf, multi_task=False, loss_type=mae\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path('../config'))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f'exp: {exp} | run_mode={config.run_mode}, multi_task={config.multi_task}, loss_type={config.loss_type}')\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir = config.input_path\n",
    "\n",
    "valid_df = pl.read_parquet(valid_dir / \"18.parquet\")\n",
    "loc_df = pl.read_parquet(valid_dir / \"18_loc.parquet\")\n",
    "valid_df = valid_df.join(loc_df, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "# # Match the data types of valid_df with train_df\n",
    "# exprs = []\n",
    "# for col in train_df.columns:\n",
    "#     if col in valid_df.columns:\n",
    "#         exprs.append(pl.col(col).cast(train_df[col].dtype))\n",
    "# valid_df = valid_df.with_columns(exprs)\n",
    "# valid_df.write_parquet(valid_dir / \"18_shrinked.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>sample_id</th><th>lat</th><th>lon</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>625000.0</td><td>625000.0</td><td>625000.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>1.1029e7</td><td>0.000131</td><td>180.00144</td></tr><tr><td>&quot;std&quot;</td><td>180422.103459</td><td>38.874559</td><td>103.671038</td></tr><tr><td>&quot;min&quot;</td><td>1.071652e7</td><td>-82.058859</td><td>5.633247</td></tr><tr><td>&quot;25%&quot;</td><td>1.087277e7</td><td>-32.585029</td><td>84.366753</td></tr><tr><td>&quot;50%&quot;</td><td>1.102902e7</td><td>4.434555</td><td>185.633247</td></tr><tr><td>&quot;75%&quot;</td><td>1.1185269e7</td><td>32.585029</td><td>275.633247</td></tr><tr><td>&quot;max&quot;</td><td>1.1341519e7</td><td>82.058859</td><td>354.366753</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 4)\n",
       "┌────────────┬───────────────┬────────────┬────────────┐\n",
       "│ statistic  ┆ sample_id     ┆ lat        ┆ lon        │\n",
       "│ ---        ┆ ---           ┆ ---        ┆ ---        │\n",
       "│ str        ┆ f64           ┆ f64        ┆ f64        │\n",
       "╞════════════╪═══════════════╪════════════╪════════════╡\n",
       "│ count      ┆ 625000.0      ┆ 625000.0   ┆ 625000.0   │\n",
       "│ null_count ┆ 0.0           ┆ 0.0        ┆ 0.0        │\n",
       "│ mean       ┆ 1.1029e7      ┆ 0.000131   ┆ 180.00144  │\n",
       "│ std        ┆ 180422.103459 ┆ 38.874559  ┆ 103.671038 │\n",
       "│ min        ┆ 1.071652e7    ┆ -82.058859 ┆ 5.633247   │\n",
       "│ 25%        ┆ 1.087277e7    ┆ -32.585029 ┆ 84.366753  │\n",
       "│ 50%        ┆ 1.102902e7    ┆ 4.434555   ┆ 185.633247 │\n",
       "│ 75%        ┆ 1.1185269e7   ┆ 32.585029  ┆ 275.633247 │\n",
       "│ max        ┆ 1.1341519e7   ┆ 82.058859  ┆ 354.366753 │\n",
       "└────────────┴───────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.select('sample_id', 'lat', 'lon').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.run_mode = 'full'\n",
    "config.multi_task = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] start [0.5GB(4.1%)]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Data Loading...'):\n",
    "    dpr = DataProvider(config)\n",
    "    train_df, test_df = dpr.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature Engineering...] start [66.5GB(38.1%)]\n",
      "[Feature Engineering...] done [69.4GB(30.2%)(+2.975GB)] 27.8554 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Feature Engineering...'):\n",
    "    fer = FeatureEngineering(config)\n",
    "    train_df = fer.feature_engineering(train_df)\n",
    "    test_df = fer.feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scaling and Clipping Features...] start [69.4GB(30.2%)]\n",
      "[Scaling and Clipping Features...] done [34.4GB(22.5%)(-35.001GB)] 36.5791 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Scaling and Clipping Features...'):\n",
    "    ppr = Preprocessor(config)\n",
    "    train_df, test_df = ppr.scaling(train_df, test_df)\n",
    "    input_cols, target_cols = ppr.input_cols, ppr.target_cols\n",
    "    if config.task_type == 'grid_pred':\n",
    "        train_df = train_df.drop(target_cols)\n",
    "\n",
    "    valid_df = train_df.filter(pl.col('fold') == 0)\n",
    "    train_df = train_df.filter(pl.col('fold') != 0)\n",
    "    valid_df, input_clip_dict = clipping_input(train_df, valid_df, input_cols)\n",
    "    test_df, _ = clipping_input(None, test_df, input_cols, input_clip_dict)\n",
    "    pickle.dump(input_clip_dict, open(config.output_path / 'input_clip_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Converting to arrays for NN...] start [34.4GB(22.5%)]\n",
      "[Converting to arrays for NN...] done [53.5GB(40.7%)(+19.076GB)] 179.8489 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Converting to arrays for NN...'):\n",
    "    array_data = ppr.convert_numpy_array(train_df, valid_df, test_df)\n",
    "    del train_df, valid_df, test_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HF Data\n",
    "if config.run_mode == 'hf':\n",
    "    with TimeUtil.timer('HF Data Preprocessing...'):\n",
    "        hf_ppr = HFPreprocessor(config)\n",
    "        # hf_pcr.preprocess_data()\n",
    "        # hf_pcr.convert_numpy_array(near_target=False)\n",
    "        # del train_loader; gc.collect()\n",
    "        # train_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Torch DataLoader...] start [53.5GB(40.7%)]\n",
      "[Creating Torch DataLoader...] done [53.5GB(40.6%)(+0.000GB)] 0.0876 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Creating Torch DataLoader...'):\n",
    "    train_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['train_ids'],\n",
    "        array_data['X_train'],\n",
    "        array_data['y_train'],\n",
    "        is_train=True\n",
    "    )\n",
    "    valid_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['valid_ids'],\n",
    "        array_data['X_valid'],\n",
    "        array_data['y_valid'],\n",
    "        is_train=False\n",
    "    )\n",
    "    test_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['test_ids'],\n",
    "        array_data['X_test'],\n",
    "        is_train=False\n",
    "    )\n",
    "    del array_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Literal, Tuple\n",
    "\n",
    "import loguru\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.train import ComponentFactory\n",
    "from src.train.train_utils import AverageMeter\n",
    "from src.utils import clean_message\n",
    "from src.utils.competition_utils import evaluate_metric, get_io_columns, get_sub_factor\n",
    "from src.utils.constant import (\n",
    "    PP_TARGET_COLS,\n",
    "    SCALER_TARGET_COLS,\n",
    "    TARGET_MIN_MAX,\n",
    "    VERTICAL_TARGET_COLS,\n",
    ")\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, save_suffix: str = \"\"):\n",
    "        self.config = config\n",
    "        self.eval_step = config.eval_step[config.run_mode]\n",
    "        self.logger = logger\n",
    "        self.save_suffix = save_suffix\n",
    "        self.detail_pbar = True\n",
    "\n",
    "        self.model = ComponentFactory.get_model(config)\n",
    "        self.model = self.model.to(config.device)\n",
    "        n_device = torch.cuda.device_count()\n",
    "        if n_device > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.loss_fn = ComponentFactory.get_loss(config)\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "\n",
    "        _, self.target_cols = get_io_columns(config)\n",
    "        self.model_target_cols = self.get_model_target_cols()\n",
    "        self.factor_dict = get_sub_factor(config.input_path, old=False)\n",
    "        self.old_factor_dict = get_sub_factor(config.input_path, old=True)\n",
    "\n",
    "        self.y_numerators = np.load(\n",
    "            config.output_path / f\"y_numerators_{config.target_scale_method}.npy\"\n",
    "        )\n",
    "        self.y_denominators = np.load(\n",
    "            config.output_path / f\"y_denominators_{config.target_scale_method}.npy\"\n",
    "        )\n",
    "        self.target_min_max = [TARGET_MIN_MAX[col] for col in self.target_cols]\n",
    "\n",
    "        self.valid_ids = None\n",
    "        self.test_ids = None\n",
    "        self.valid_pp_df = None\n",
    "        self.test_pp_df = None\n",
    "        self.pp_run = True\n",
    "        self.pp_y_cols = PP_TARGET_COLS\n",
    "        self.pp_x_cols = [col.replace(\"ptend\", \"state\") for col in self.pp_y_cols]\n",
    "\n",
    "        self.best_score_dict = defaultdict(lambda: (-1, -np.inf))\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        colwise_mode: bool = True,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str = \"\",\n",
    "        retrain_best_score: float = -np.inf,\n",
    "        eval_only: bool = False,\n",
    "    ):\n",
    "        if eval_only:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            eval_method = \"colwise\" if colwise_mode else \"single\"\n",
    "            score, cw_score, preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=eval_method\n",
    "            )\n",
    "            self.save_oof_df(self.valid_ids, preds)\n",
    "            return score, cw_score, -1\n",
    "\n",
    "        self.optimizer = ComponentFactory.get_optimizer(self.config, self.model)\n",
    "        self.scheduler = ComponentFactory.get_scheduler(\n",
    "            self.config, self.optimizer, steps_per_epoch=len(train_loader)\n",
    "        )\n",
    "        global_step = 0\n",
    "        eval_count = 0\n",
    "        best_score = -np.inf\n",
    "\n",
    "        if retrain:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\")\n",
    "            )\n",
    "            weight_numbers = [\n",
    "                int(file.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "                for file in list(self.config.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "            ]\n",
    "            eval_count = sorted(weight_numbers)[-1] + 1\n",
    "            best_score = retrain_best_score\n",
    "\n",
    "        # 学習ループの開始\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            iterations = (\n",
    "                tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            )\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(data, calc_loss=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % self.eval_step == 0:\n",
    "                    score, _, preds, update_num = self.valid_evaluate(\n",
    "                        valid_loader,\n",
    "                        current_epoch=epoch,\n",
    "                        eval_count=eval_count,\n",
    "                        eval_method=\"single\",\n",
    "                    )\n",
    "                    if colwise_mode and update_num > 0:\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            self.config.output_path\n",
    "                            / f\"model{self.save_suffix}_eval{eval_count}.pth\",\n",
    "                        )\n",
    "\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preds = preds\n",
    "                        best_epochs = epoch\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            self.config.output_path / f\"model{self.save_suffix}_best.pth\",\n",
    "                        )\n",
    "\n",
    "                    eval_count += 1\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.5f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.5e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "        if colwise_mode:\n",
    "            self.remove_unuse_weights()\n",
    "            best_score, best_cw_score, best_preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=\"colwise\"\n",
    "            )\n",
    "\n",
    "        self.save_oof_df(self.valid_ids, best_preds)\n",
    "        return best_score, best_cw_score, best_epochs\n",
    "\n",
    "    def valid_evaluate(\n",
    "        self,\n",
    "        valid_loader: DataLoader,\n",
    "        current_epoch: int,\n",
    "        eval_count: int,\n",
    "        eval_method: Literal[\"single\", \"colwise\"] = \"single\",\n",
    "    ):\n",
    "        if self.valid_ids is None:\n",
    "            self.valid_ids = valid_loader.dataset.ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            load_best_weight = True if eval_count == -1 else False\n",
    "            preds = self.inference_loop(\n",
    "                valid_loader, mode=\"valid\", load_best_weight=load_best_weight\n",
    "            )\n",
    "        elif eval_method == \"colwise\":\n",
    "            preds = self.inference_loop_colwise(valid_loader, \"valid\", self.best_score_dict)\n",
    "\n",
    "        labels = valid_loader.dataset.y\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            labels = self.convert_target_3dim_to_2dim(labels)\n",
    "        preds = self.restore_pred(preds)\n",
    "        labels = self.restore_pred(labels)\n",
    "\n",
    "        if self.pp_run and self.valid_pp_df is None:\n",
    "            self.load_postprocess_input(\"valid\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"valid\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        eval_idx = [\n",
    "            i for i, col in enumerate(self.target_cols) if self.factor_dict[col] != 0\n",
    "        ]  # factor_dictの値が0のものは自動でR2=1になるようにする\n",
    "        score, indiv_scores = evaluate_metric(preds, labels, eval_idx=eval_idx)\n",
    "        cw_score, update_num = self.update_best_score(indiv_scores, eval_count)\n",
    "\n",
    "        message = f\"\"\"\n",
    "            [Valid] :\n",
    "                Epoch={current_epoch},\n",
    "                Loss={self.valid_loss.avg:.5f},\n",
    "                Score={score:.5f},\n",
    "                Best Col-Wise Score={cw_score:.5f}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, cw_score, preds, update_num\n",
    "\n",
    "    def test_predict(\n",
    "        self, test_loader: DataLoader, eval_method: Literal[\"single\", \"colwise\"] = \"single\"\n",
    "    ):\n",
    "        if self.test_ids is None:\n",
    "            self.test_ids = test_loader.dataset.ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            preds = self.inference_loop(test_loader, mode=\"test\", load_best_weight=True)\n",
    "        elif eval_method == \"colwise\":\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            preds = self.inference_loop_colwise(test_loader, \"test\", self.best_score_dict)\n",
    "\n",
    "        preds = self.restore_pred(preds)\n",
    "        if self.pp_run and self.test_pp_df is None:\n",
    "            self.load_postprocess_input(\"test\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"test\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        pred_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        pred_df = pred_df.with_columns(sample_id=pl.Series(self.test_ids))\n",
    "        return pred_df\n",
    "\n",
    "    def inference_loop(\n",
    "        self,\n",
    "        eval_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        load_best_weight: bool = False,\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        # テストデータを推論するときはbest_weightを読み込む\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\")\n",
    "            )\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = (\n",
    "                tqdm(eval_loader, total=len(eval_loader)) if self.detail_pbar else eval_loader\n",
    "            )\n",
    "            for data in iterations:\n",
    "                if mode == \"valid\":\n",
    "                    out, loss = self.forward_step(data, calc_loss=True)\n",
    "                    self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                elif mode == \"test\":\n",
    "                    out, _ = self.forward_step(data, calc_loss=False)\n",
    "                preds.append(out.detach().cpu().numpy())\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        return preds\n",
    "\n",
    "    def inference_loop_colwise(\n",
    "        self,\n",
    "        test_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        best_score_dict: dict[str, tuple[int, float]],\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        selected_counts = list(set([eval_count for eval_count, _ in best_score_dict.values()]))\n",
    "        all_preds = np.zeros((len(test_loader.dataset), len(self.target_cols)))\n",
    "        for eval_count in tqdm(selected_counts):\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    self.config.output_path / f\"model{self.save_suffix}_eval{eval_count}.pth\"\n",
    "                )\n",
    "            )\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                iterations = (\n",
    "                    tqdm(test_loader, total=len(test_loader)) if self.detail_pbar else test_loader\n",
    "                )\n",
    "                for data in iterations:\n",
    "                    if mode == \"valid\":\n",
    "                        out, loss = self.forward_step(data, calc_loss=True)\n",
    "                        self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                    elif mode == \"test\":\n",
    "                        out, _ = self.forward_step(data, calc_loss=False)\n",
    "                    preds.append(out.detach().cpu().numpy())\n",
    "            preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "            target_cols = [\n",
    "                col for col, (count, _) in best_score_dict.items() if count == eval_count\n",
    "            ]\n",
    "            for col in target_cols:\n",
    "                idx = self.target_cols.index(col)\n",
    "                all_preds[:, idx] = preds[:, idx]\n",
    "        return all_preds\n",
    "\n",
    "    def update_best_score(self, indiv_scores: list[float], eval_count: int):\n",
    "        update_num = 0\n",
    "        for col, score in zip(self.target_cols, indiv_scores):\n",
    "            if score > self.best_score_dict[col][1] and eval_count != -1:\n",
    "                self.best_score_dict[col] = (eval_count, score)\n",
    "                update_num += 1\n",
    "\n",
    "        best_cw_score = (\n",
    "            np.sum([score for _, score in self.best_score_dict.values()])\n",
    "            + (368 - len(self.target_cols))\n",
    "        ) / 368\n",
    "        if update_num > 0 and eval_count != -1:\n",
    "            pickle.dump(\n",
    "                dict(self.best_score_dict),\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"wb\"),\n",
    "            )\n",
    "        return best_cw_score, update_num\n",
    "\n",
    "    def remove_unuse_weights(self):\n",
    "        selected_counts = set([v[0] for v in self.best_score_dict.values()])\n",
    "        weight_paths = list(self.config.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "        for path in weight_paths:\n",
    "            eval_count = int(path.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "            if eval_count not in selected_counts:\n",
    "                path.unlink()\n",
    "\n",
    "    def forward_step(self, data: torch.Tensor, calc_loss: bool = True):\n",
    "        if calc_loss:\n",
    "            x, y = data\n",
    "            x, y = x.to(self.config.device), y.to(self.config.device)\n",
    "            out = self.model(x)\n",
    "            loss = self.loss_fn(out, y)\n",
    "        else:\n",
    "            x = data[0]\n",
    "            x = x.to(self.config.device)\n",
    "            out = self.model(x)\n",
    "            loss = None\n",
    "\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            out = self.convert_target_3dim_to_2dim(out)\n",
    "        return out, loss\n",
    "\n",
    "    def convert_target_3dim_to_2dim(\n",
    "        self, y: np.ndarray | torch.Tensor\n",
    "    ) -> np.ndarray | torch.Tensor:\n",
    "        y_v = y[:, :, : len(VERTICAL_TARGET_COLS)]\n",
    "        y_s = y[:, :, len(VERTICAL_TARGET_COLS) :]\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y_v = np.transpose(y_v, (0, 2, 1)).reshape(y.shape[0], -1)\n",
    "            y_s = y_s.mean(axis=1)\n",
    "            y = np.concatenate([y_v, y_s], axis=-1)\n",
    "        elif isinstance(y, torch.Tensor):\n",
    "            y_v = y_v.permute(0, 2, 1).reshape(y.size(0), -1)\n",
    "            y_s = y_s.mean(dim=1)\n",
    "            y = torch.cat([y_v, y_s], dim=-1)\n",
    "        y = self.alignment_target_idx(y)\n",
    "        return y\n",
    "\n",
    "    def alignment_target_idx(self, y: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "        \"\"\"\n",
    "        target_colsとモデルの出力の順番を合わせる\n",
    "        \"\"\"\n",
    "        align_order = [self.model_target_cols.index(col) for col in self.target_cols]\n",
    "        assert len(y.shape) == 2\n",
    "        y = y[:, align_order]\n",
    "        return y\n",
    "\n",
    "    def get_model_target_cols(self):\n",
    "        model_target_cols = []\n",
    "        for col in VERTICAL_TARGET_COLS:\n",
    "            model_target_cols.extend([f\"{col}_{i}\" for i in range(60)])\n",
    "        for col in SCALER_TARGET_COLS:\n",
    "            model_target_cols.append(col)\n",
    "        return model_target_cols\n",
    "\n",
    "    def restore_pred(self, preds: np.ndarray):\n",
    "        return preds * self.y_denominators + self.y_numerators\n",
    "\n",
    "    def clipping_pred(self, preds: np.ndarray):\n",
    "        for i in range(preds.shape[1]):\n",
    "            preds[:, i] = np.clip(preds[:, i], self.target_min_max[i][0], self.target_min_max[i][1])\n",
    "        return preds\n",
    "\n",
    "    def save_oof_df(self, sample_ids: np.ndarray, preds: np.ndarray):\n",
    "        oof_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        oof_df = oof_df.with_columns(sample_id=pl.Series(sample_ids))\n",
    "        oof_df.write_parquet(self.config.oof_path / f\"oof{self.save_suffix}.parquet\")\n",
    "\n",
    "    def postprocess(self, preds: np.ndarray, run_type: Literal[\"valid\", \"test\"]):\n",
    "        pp_x = self.valid_pp_df if run_type == \"valid\" else self.test_pp_df\n",
    "        for x_col, y_col in zip(self.pp_x_cols, self.pp_y_cols):\n",
    "            if y_col in self.target_cols:\n",
    "                idx = self.target_cols.index(y_col)\n",
    "                old_factor = self.old_factor_dict[y_col] if self.config.mul_old_factor else 1\n",
    "                preds[:, idx] = (-1 * pp_x[x_col].to_numpy() / 1200) * old_factor\n",
    "        return preds\n",
    "\n",
    "    def load_postprocess_input(self, data_type: Literal[\"valid\", \"test\"]):\n",
    "        if data_type == \"valid\":\n",
    "            valid_path = (\n",
    "                self.config.input_path / \"18_shrinked.parquet\"\n",
    "                if self.config.shared_valid\n",
    "                else self.config.input_path / \"train_shrinked.parquet\"\n",
    "            )\n",
    "            self.valid_pp_df = (\n",
    "                pl.scan_parquet(valid_path)\n",
    "                .select([\"sample_id\"] + self.pp_x_cols)\n",
    "                .filter(pl.col(\"sample_id\").is_in(self.valid_ids))\n",
    "                .collect()\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.valid_ids})\n",
    "            self.valid_pp_df = id_df.join(self.valid_pp_df, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "        elif data_type == \"test\":\n",
    "            self.test_pp_df = pl.read_parquet(\n",
    "                self.config.input_path / \"test_shrinked.parquet\",\n",
    "                columns=[\"sample_id\"] + self.pp_x_cols,\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.test_ids})\n",
    "            self.test_pp_df = id_df.join(self.test_pp_df, on=\"sample_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 28 0.6665454148778593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32326/490323976.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c711ffdff347458d1bfd267e6e40a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398a841f9ca0471aaa3c2499b851c76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94658a8c2804067a54b50fc0806d625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-10 14:24:55\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.16988, Score=0.66968, Best Col-Wise Score=0.67182\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m oof_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolwise_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrain_weight_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_eval27\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretrain_best_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6665454148778593\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_only=True\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[97], line 118\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, valid_loader, colwise_mode, retrain, retrain_weight_name, retrain_best_score, eval_only)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, n\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    119\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof_df = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    colwise_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32326/383357159.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc369fc74294b2a81a589c43bd4655b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = trainer.test_predict(test_loader, eval_method=\"colwise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "import loguru\n",
    "from typing import Literal\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from src.utils.competition_utils import get_sub_factor, get_io_columns\n",
    "\n",
    "class PostProcess:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, additional: bool = True):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.additional = additional\n",
    "\n",
    "        _, self.target_cols = get_io_columns(config)\n",
    "        self.old_factor_dict = get_sub_factor(config.input_path, old=True)\n",
    "        self.sub_cols = pl.read_parquet(config.input_path / 'sample_submission.parquet', n_rows=1).columns\n",
    "\n",
    "        self.pp_x_cols = [f'state_q0002_{i}' for i in range(12, 27)]\n",
    "        self.pp_y_cols = [f'ptend_q0002_{i}' for i in range(12, 27)]\n",
    "        self.valid_pp_df = pl.read_parquet(\n",
    "            config.input_path / '18_shrinked.parquet',\n",
    "            columns=['sample_id'] + self.pp_x_cols\n",
    "        )\n",
    "        self.test_pp_df = pl.read_parquet(\n",
    "            config.input_path / 'test_shrinked.parquet',\n",
    "            columns=['sample_id'] + self.pp_x_cols\n",
    "        )\n",
    "\n",
    "        add_pp_y_cols = (\n",
    "            [f'ptend_q0002_{i}' for i in range(60)] +\n",
    "            [f'ptend_q0003_{i}' for i in range(60)]\n",
    "        )\n",
    "        self.add_pp_y_cols = [col for col in add_pp_y_cols if col in self.target_cols]\n",
    "        self.add_pp_x_cols = [col.replace('ptend', 'state') for col in self.add_pp_y_cols]\n",
    "        self.add_valid_pp_df = pl.read_parquet(\n",
    "            config.input_path / '18_shrinked.parquet',\n",
    "            columns=self.sub_cols + self.add_pp_x_cols\n",
    "        )\n",
    "        self.add_test_pp_df = pl.read_parquet(\n",
    "            config.input_path / 'test_shrinked.parquet',\n",
    "            columns=['sample_id'] + self.add_pp_x_cols\n",
    "        )\n",
    "        self.th_dict = None\n",
    "\n",
    "    def postprocess(self, oof_df: pl.DataFrame, sub_df: pl.DataFrame):\n",
    "        oof_df = self.complement_columns(oof_df)\n",
    "        oof_df = self.reverse_sub_factor(oof_df)\n",
    "        oof_df = self.replace_postprocess(oof_df, 'oof')\n",
    "\n",
    "        sub_df = self.complement_columns(sub_df)\n",
    "        sub_df = self.reverse_sub_factor(sub_df)\n",
    "        sub_df = self.replace_postprocess(sub_df, 'sub')\n",
    "\n",
    "        if self.additional:\n",
    "            oof_df = self.additional_postprocess(oof_df, 'oof')\n",
    "            sub_df = self.additional_postprocess(sub_df, 'sub')\n",
    "\n",
    "        oof_df = self.create_oof_df(oof_df)\n",
    "        sub_df = self.create_sub_df(sub_df)\n",
    "        return oof_df, sub_df\n",
    "\n",
    "    def complement_columns(self, pred_df: pl.DataFrame):\n",
    "        lack_cols = list(set(self.sub_cols) - set(pred_df.columns))\n",
    "        for col in lack_cols:\n",
    "            pred_df = pred_df.with_columns([pl.lit(0).alias(col)])\n",
    "        return pred_df\n",
    "\n",
    "    def reverse_sub_factor(self, pred_df: pl.DataFrame):\n",
    "        if self.config.mul_old_factor:\n",
    "            exprs = []\n",
    "            for col in self.target_cols:\n",
    "                if self.old_factor_dict[col] != 0:\n",
    "                    exprs.append((pl.col(col) / self.old_factor_dict[col]).alias(col))\n",
    "\n",
    "            pred_df = pred_df.with_columns(exprs)\n",
    "        return pred_df\n",
    "\n",
    "    def replace_postprocess(self, pred_df: pl.DataFrame, pred_type: Literal['oof', 'sub']):\n",
    "        pp_df = self.valid_pp_df if pred_type == 'oof' else self.test_pp_df\n",
    "        pred_df = pred_df.join(pp_df, on=['sample_id'], how='left')\n",
    "\n",
    "        exprs = []\n",
    "        for x_col, y_col in zip(self.pp_x_cols, self.pp_y_cols):\n",
    "            exprs.append((-1 * pl.col(x_col) / 1200).alias(y_col))\n",
    "        pred_df = pred_df.with_columns(exprs)\n",
    "        pred_df = pred_df.drop(self.pp_x_cols)\n",
    "        return pred_df\n",
    "\n",
    "    def additional_postprocess(self, pred_df: pl.DataFrame, pred_type: Literal['oof', 'sub']):\n",
    "        pp_df = self.add_valid_pp_df if pred_type == 'oof' else self.add_test_pp_df\n",
    "        pred_df = pred_df.join(pp_df, on=['sample_id'], how='left', suffix='_gt')\n",
    "        exprs = []\n",
    "        for x_col, y_col in zip(self.add_pp_x_cols, self.add_pp_y_cols):\n",
    "            exprs.append((pl.col(x_col) + pl.col(y_col) * 1200).alias(f'{x_col}_next'))\n",
    "        pred_df = pred_df.with_columns(exprs)\n",
    "\n",
    "        if pred_type == 'oof':\n",
    "            self.tuning_threshold(pred_df)\n",
    "\n",
    "        assert self.th_dict is not None # oofから実行する必要がある\n",
    "        exprs = []\n",
    "        for y_col, (best_th, _) in self.th_dict.items():\n",
    "            x_col = y_col.replace('ptend', 'state')\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(f'{x_col}_next') < best_th)\n",
    "                .then(-1 * pl.col(x_col) / 1200)\n",
    "                .otherwise(pl.col(y_col))\n",
    "                .alias(y_col)\n",
    "            )\n",
    "        pred_df = pred_df.with_columns(exprs)\n",
    "\n",
    "        if pred_type == 'oof':\n",
    "            scores = []\n",
    "            for col in self.target_cols:\n",
    "                score = r2_score(pred_df[f'{col}_gt'].to_numpy(), pred_df[col].to_numpy())\n",
    "                scores.append(score)\n",
    "            total_score = (np.sum(scores) + (368 - len(scores))) / 368\n",
    "            self.logger.info(f'After Additional Postprocess: {total_score:.5f}')\n",
    "\n",
    "        drop_cols = (\n",
    "            self.add_pp_x_cols +\n",
    "            [f'{col}_next' for col in self.add_pp_x_cols] +\n",
    "            [col for col in pred_df.columns if '_gt' in col]\n",
    "        )\n",
    "        pred_df = pred_df.drop(drop_cols)\n",
    "        return pred_df\n",
    "\n",
    "    def tuning_threshold(self, pred_df: pl.DataFrame):\n",
    "        iterations = tqdm(zip(self.add_pp_x_cols, self.add_pp_y_cols), total=len(self.add_pp_x_cols))\n",
    "        for x_col, y_col in iterations:\n",
    "            best_score = r2_score(pred_df[f'{y_col}_gt'].to_numpy(), pred_df[y_col].to_numpy())\n",
    "            best_th = None\n",
    "            for th_base in [0, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5]:\n",
    "                for corr in range(1, 10):\n",
    "                    if th_base == 0 and corr >= 2:\n",
    "                        break\n",
    "\n",
    "                    th = th_base * corr\n",
    "                    preds = pred_df.select(\n",
    "                        pl.when(pl.col(f'{x_col}_next') < th)\n",
    "                        .then(-1 * pl.col(x_col) / 1200)\n",
    "                        .otherwise(pl.col(y_col))\n",
    "                    ).to_numpy()\n",
    "\n",
    "                    truths = pred_df[f'{y_col}_label'].to_numpy()\n",
    "                    score = r2_score(truths, preds)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_th = th\n",
    "\n",
    "            if best_th is not None:\n",
    "                self.th_dict[y_col] = (best_th, best_score)\n",
    "\n",
    "\n",
    "    def create_oof_df(self, oof_df: pl.DataFrame):\n",
    "        oof_df = oof_df.select(self.sub_cols)\n",
    "        oof_df.write_parquet(self.config.oof_path / 'oof_pp.parquet')\n",
    "        return oof_df\n",
    "\n",
    "    def create_sub_df(self, sub_df: pl.DataFrame):\n",
    "        sub_df = sub_df.with_columns(sample_id = pl.concat_str([pl.lit('test_'), pl.col('sample_id')]))\n",
    "        sub_df = sub_df.select(self.sub_cols)\n",
    "        sub_df.write_csv(self.config.output_path / 'submission_pp.csv')\n",
    "        return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
