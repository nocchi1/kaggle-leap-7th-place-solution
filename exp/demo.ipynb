{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import seed_everything, get_logger, get_config, TimeUtil\n",
    "from src.utils.competition_utils import clipping_input\n",
    "from src.data import DataProvider, FeatureEngineering, Preprocessor, HFPreprocessor\n",
    "from src.train import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = '146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-09 06:30:46\u001b[0m | \u001b[1mINFO ] exp: 146 | run_mode=hf, multi_task=False, loss_type=mae\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path('../config'))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f'exp: {exp} | run_mode={config.run_mode}, multi_task={config.multi_task}, loss_type={config.loss_type}')\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.run_mode = 'debug'\n",
    "config.multi_task = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] start [0.5GB(16.8%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] done [56.7GB(20.3%)(+56.153GB)] 13.4642 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Data Loading...'):\n",
    "    dpr = DataProvider(config)\n",
    "    train_df, test_df = dpr.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature Engineering...] start [56.7GB(20.1%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature Engineering...] done [58.4GB(19.4%)(+1.793GB)] 7.4836 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Feature Engineering...'):\n",
    "    fer = FeatureEngineering(config)\n",
    "    train_df = fer.feature_engineering(train_df)\n",
    "    test_df = fer.feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scaling and Clipping Features...] start [58.4GB(19.4%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scaling and Clipping Features...] done [58.3GB(18.0%)(-0.135GB)] 3.8249 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Scaling and Clipping Features...'):\n",
    "    ppr = Preprocessor(config)\n",
    "    train_df, test_df = ppr.scaling(train_df, test_df)\n",
    "    input_cols, target_cols = ppr.input_cols, ppr.target_cols\n",
    "    if config.task_type == 'grid_pred':\n",
    "        train_df = train_df.drop(target_cols)\n",
    "\n",
    "    valid_df = train_df.filter(pl.col('fold') == 0)\n",
    "    train_df = train_df.filter(pl.col('fold') != 0)\n",
    "    valid_df, input_clip_dict = clipping_input(train_df, valid_df, input_cols)\n",
    "    test_df, _ = clipping_input(None, test_df, input_cols, input_clip_dict)\n",
    "    pickle.dump(input_clip_dict, open(config.output_path / 'input_clip_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Converting to arrays for NN...] start [58.3GB(17.6%)]\n",
      "[Converting to arrays for NN...] done [69.3GB(21.7%)(+10.971GB)] 46.5510 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Converting to arrays for NN...'):\n",
    "    array_data = ppr.convert_numpy_array(train_df, valid_df, test_df)\n",
    "    del train_df, valid_df, test_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HF Data\n",
    "if config.run_mode == 'hf':\n",
    "    with TimeUtil.timer('HF Data Preprocessing...'):\n",
    "        hf_ppr = HFPreprocessor(config)\n",
    "        # hf_pcr.preprocess_data()\n",
    "        # hf_pcr.convert_numpy_array(near_target=False)\n",
    "        # del train_loader; gc.collect()\n",
    "        # train_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Torch DataLoader...] start [69.3GB(21.4%)]\n",
      "[Creating Torch DataLoader...] done [69.3GB(21.4%)(+0.000GB)] 0.1442 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Creating Torch DataLoader...'):\n",
    "    train_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['train_ids'],\n",
    "        array_data['X_train'],\n",
    "        array_data['y_train'],\n",
    "        is_train=True\n",
    "    )\n",
    "    valid_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['valid_ids'],\n",
    "        array_data['X_valid'],\n",
    "        array_data['y_valid'],\n",
    "        is_train=False\n",
    "    )\n",
    "    test_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['test_ids'],\n",
    "        array_data['X_test'],\n",
    "        is_train=False\n",
    "    )\n",
    "    del array_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from src.utils.constant import VERTICAL_INPUT_COLS, ADDITIONAL_VERTICAL_INPUT_COLS\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for all LEAP models\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_vertical = len(VERTICAL_INPUT_COLS) + len(ADDITIONAL_VERTICAL_INPUT_COLS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        v_x = x[:, :, :self.num_vertical]\n",
    "        diff_feat = self.calc_diff_feats(v_x)\n",
    "        x = torch.cat([x, diff_feat], dim=-1)\n",
    "        return x\n",
    "\n",
    "    def calc_lag_features(self, x):\n",
    "        feat = []\n",
    "        # Calculate lag features for both forward and backward directions\n",
    "        for t in [1, 2, 3, 4, 5]:\n",
    "            x_f = torch.cat(\n",
    "                [torch.zeros(x.size(0), t, x.size(2), device=x.device), x[:, : (60 - t), :]], dim=1\n",
    "            )\n",
    "            feat.append(x_f)\n",
    "            x_b = torch.cat(\n",
    "                [x[:, t:, :], torch.zeros(x.size(0), t, x.size(2), device=x.device)], dim=1\n",
    "            )\n",
    "            feat.append(x_b)\n",
    "        feat = torch.cat(feat, dim=-1)\n",
    "        return feat\n",
    "\n",
    "    def calc_diff_feats(self, x):\n",
    "        # Forward direction diff\n",
    "        forward_diff = torch.diff(x, dim=1)\n",
    "        forward_diff = torch.cat([torch.zeros_like(x[:, 0, :]).unsqueeze(1), forward_diff], dim=1)\n",
    "        # Forward direction second diff\n",
    "        forward_diff2 = torch.diff(forward_diff, dim=1)\n",
    "        forward_diff2 = torch.cat([torch.zeros_like(x[:, 0, :]).unsqueeze(1), forward_diff2], dim=1)\n",
    "        # Backward direction diff\n",
    "        backward_diff = torch.diff(x.flip(1), dim=1).flip(1)\n",
    "        backward_diff = torch.cat([backward_diff, torch.zeros_like(x[:, 0, :]).unsqueeze(1)], dim=1)\n",
    "        # Backward direction second diff\n",
    "        backward_diff2 = torch.diff(backward_diff.flip(1), dim=1).flip(1)\n",
    "        backward_diff2 = torch.cat(\n",
    "            [backward_diff2, torch.zeros_like(x[:, 0, :]).unsqueeze(1)], dim=1\n",
    "        )\n",
    "        feat = torch.cat([forward_diff, forward_diff2, backward_diff, backward_diff2], dim=-1)\n",
    "        return feat\n",
    "\n",
    "    def calc_moving_feats(self, x):\n",
    "        feat = []\n",
    "        x = x.transpose(1, 2)  # (batch, hidden, sequence)\n",
    "        # Moving statistics\n",
    "        for w in [3, 5, 7, 15, 29]:\n",
    "            # Mean, max, min\n",
    "            feat.append(F.avg_pool1d(x, w, stride=1, padding=w // 2))\n",
    "            feat.append(F.max_pool1d(x, w, stride=1, padding=w // 2))\n",
    "            feat.append(-1 * F.max_pool1d(-1 * x, w, stride=1, padding=w // 2))\n",
    "            # Standard deviation\n",
    "            x_mean = F.avg_pool1d(x, w, stride=1, padding=w // 2)\n",
    "            x_diff = (x - x_mean) ** 2\n",
    "            x_std = F.avg_pool1d(x_diff, w, stride=1, padding=w // 2).sqrt()\n",
    "            x_std = torch.where(torch.isinf(x_std) | torch.isnan(x_std), 0, x_std)\n",
    "            feat.append(x_std)\n",
    "\n",
    "        # Global statistics\n",
    "        feat.append(x.mean(dim=2, keepdim=True).repeat(1, 1, x.size(2)))\n",
    "        feat.append(x.max(dim=2, keepdim=True).values.repeat(1, 1, x.size(2)))\n",
    "        feat.append(x.min(dim=2, keepdim=True).values.repeat(1, 1, x.size(2)))\n",
    "        x_std = x.std(dim=2, keepdim=True).repeat(1, 1, x.size(2))\n",
    "        x_std = torch.where(torch.isinf(x_std) | torch.isnan(x_std), 0, x_std)\n",
    "        feat.append(x_std)\n",
    "\n",
    "        feat = torch.cat(feat, dim=1)\n",
    "        feat = feat.transpose(1, 2)\n",
    "        return feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.modules import VerticalEncoding, ResNetBlock, ConvExtractor, LSTMBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in valid_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Sequential(\n",
    "    nn.Conv1d(30, config.hidden_dim, kernel_size=1, stride=1, padding=0),\n",
    "    nn.BatchNorm1d(config.hidden_dim),\n",
    "    nn.ELU(),\n",
    ")\n",
    "x = x.transpose(1, 2)\n",
    "x = embedding(x)\n",
    "x = x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 60, 256])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_id = torch.randint(0, 384, (config.eval_batch, ))\n",
    "he = VHPositionalEncoding(config.hidden_dim)\n",
    "out = he(x, g_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 60, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DSeq2Seq(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        block_num: int = 15,\n",
    "        kernel_size: int = 5,\n",
    "        multitask: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if multitask:\n",
    "            out_dim = 3 * out_dim\n",
    "\n",
    "        activation = nn.ELU()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hidden_dim, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "        )\n",
    "        self.pe = VerticalEncoding(hidden_dim, learnable=True)\n",
    "        conv_blocks = []\n",
    "        for i in range(block_num):\n",
    "            if i >= (block_num // 2) and i < (block_num - 1):\n",
    "                conv_blocks.append(\n",
    "                    ResNetBlock(hidden_dim, hidden_dim, activation=activation, kernel_size=kernel_size, inception=False)\n",
    "                )\n",
    "            else:\n",
    "                conv_blocks.append(\n",
    "                    ResNetBlock(hidden_dim, hidden_dim, activation=activation, kernel_size=kernel_size, inception=True)\n",
    "                )\n",
    "        self.conv_blocks = nn.ModuleList(conv_blocks)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, 64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, out_dim, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pe(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        out = self.head(x)\n",
    "        return out.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.torch.models.modules import (\n",
    "    SequentialStats,\n",
    "    VerticalEncoding,\n",
    "    ResNetBlock,\n",
    "    ConvExtractor,\n",
    "    LSTMBlock,\n",
    ")\n",
    "\n",
    "class Conv1DSeq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        block_num: int = 15,\n",
    "        kernel_size: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        activation = nn.ELU()\n",
    "        self.seq_stats = SequentialStats()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, hidden_dim, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "        )\n",
    "        self.pe = VerticalEncoding(hidden_dim, learnable=True)\n",
    "        conv_blocks = []\n",
    "        for i in range(block_num):\n",
    "            if i >= (block_num // 2) and i < (block_num - 1):\n",
    "                conv_blocks.append(\n",
    "                    ResNetBlock(hidden_dim, hidden_dim, activation=activation, kernel_size=kernel_size, inception=False)\n",
    "                )\n",
    "            else:\n",
    "                conv_blocks.append(\n",
    "                    ResNetBlock(hidden_dim, hidden_dim, activation=activation, kernel_size=kernel_size, inception=True)\n",
    "                )\n",
    "        self.conv_blocks = nn.ModuleList(conv_blocks)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, 64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, out_dim, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq_stats(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pe(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "        out = self.head(x)\n",
    "        return out.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loguru\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from src.utils.constant import VERTICAL_TARGET_COLS\n",
    "from src.train import ComponentFactory\n",
    "from src.train.train_utils import AverageMeter\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "\n",
    "        self.factory = ComponentFactory(config)\n",
    "        self.model = self.factory.get_model()\n",
    "        self.model = self.model.to(config.device)\n",
    "        n_device = torch.cuda.device_count()\n",
    "        if n_device > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.loss_fn = self.factory.get_loss()\n",
    "\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "\n",
    "        self.y_numerators = pickle.load(open(config.output_path / f'y_numerators_{config.target_scale_method}.npy', 'rb'))\n",
    "        self.y_denominators = pickle.load(open(config.output_path / f'y_denominators_{config.target_scale_method}.npy', 'rb'))\n",
    "\n",
    "\n",
    "    def train(self, train_loader: DataLoader, valid_loader: DataLoader, detail_pbar: bool = True):\n",
    "\n",
    "        self.optimizer = self.factory.get_optimizer()\n",
    "        self.scheduler = self.factory.get_scheduler(steps_per_epoch=len(train_loader))\n",
    "\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            iterations = tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(data, calc_loss=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % self.config.eval_step == 0:\n",
    "                    score, _, preds = self.valid_evaluate(valid_loader, epoch, eval_count)\n",
    "                    # if colwise_best_weight:\n",
    "                    #     torch.save(self.model.state_dict(), self.output_path / f'model{self.save_suffix}_eval{eval_count}.pth')\n",
    "                    # if score > best_score:\n",
    "                    #     best_score = score\n",
    "                    #     best_preds = preds\n",
    "                    #     best_epochs = epoch\n",
    "                    #     torch.save(self.model.state_dict(), self.output_path / f'model{self.save_suffix}_best.pth')\n",
    "                    # eval_count += 1\n",
    "                    # self.model.train()\n",
    "\n",
    "    def valid_evaluate(\n",
    "            self,\n",
    "            valid_loader: DataLoader,\n",
    "            current_epoch: int,\n",
    "            eval_count: int,\n",
    "            eval_colwise: bool = False,\n",
    "            load_best_weight: bool = False,\n",
    "        ):\n",
    "            if self.valid_ids is None:\n",
    "                self.valid_ids = valid_loader.dataset.sample_ids\n",
    "\n",
    "            if eval_colwise:\n",
    "                preds = self.inference_loop_colwise(valid_loader, 'valid', self.best_score_dict)\n",
    "            else:\n",
    "                preds = self.inference_loop(valid_loader, 'valid', load_best_weight)\n",
    "\n",
    "            labels = valid_loader.dataset.y\n",
    "            if self.config.target_shape == '3dim':\n",
    "                labels = self.convert_target_3dim_to_2dim(labels)\n",
    "            labels = self.restore_pred(labels)\n",
    "\n",
    "            if self.pp_run and self.valid_pp_x is None:\n",
    "                self.load_input_for_postprocess('valid')\n",
    "            if self.pp_run:\n",
    "                preds = self.postprocess(preds, run_type='valid')\n",
    "            if self.out_clip:\n",
    "                preds = self.clipping_pred(preds)\n",
    "\n",
    "            eval_idx = [i for i, col in enumerate(self.target_cols) if self.factor_dict[col] != 0]  # factor_dictの値が0のものは自動でR2=1になるようにする\n",
    "            score, indiv_score = evaluate_metric(preds, labels, individual=True, eval_idx=eval_idx)\n",
    "            save_dict = False if load_best_weight else True # 通常の学習ループの時のみbest_score_dictの保存を行う\n",
    "            colwise_score = self.update_best_score(indiv_score, eval_count, save_dict=save_dict)\n",
    "            message = f\"\"\"\n",
    "                [Valid] :\n",
    "                    Epoch={current_epoch},\n",
    "                    Loss={self.valid_loss.avg:.7f},\n",
    "                    Score={score:.5f},\n",
    "                    Best Col-Wise Score={colwise_score:.5f}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "            return score, colwise_score, preds\n",
    "\n",
    "    def forward_step(self, data: torch.Tensor, calc_loss: bool = True):\n",
    "        if calc_loss:\n",
    "            x, y = data\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            out = self.model(x)\n",
    "            if self.config.target_shape == '3dim':\n",
    "                out = self.convert_target_3dim_to_2dim(out)\n",
    "                y = self.convert_target_3dim_to_2dim(y)\n",
    "            loss = self.loss_fn(out, y)\n",
    "            return out, loss\n",
    "        else:\n",
    "            x = data\n",
    "            x = x.to(self.device)\n",
    "            out = self.model(x)\n",
    "            if self.config.target_shape == '3dim':\n",
    "                out = self.convert_target_3dim_to_2dim(out)\n",
    "            return out, None\n",
    "\n",
    "    def inference_loop(\n",
    "        self,\n",
    "        test_loader: DataLoader,\n",
    "        mode: Literal['valid', 'test'],\n",
    "        load_best_weight: bool = False\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == 'valid':\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(torch.load(self.output_path / f'model{self.save_suffix}_best.pth'))\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = tqdm(test_loader, total=len(test_loader)) if self.detail_pbar else test_loader\n",
    "            for batched in iterations:\n",
    "                if mode == 'valid':\n",
    "                    out, loss = self.forward_step(batched, calc_loss=True)\n",
    "                    self.valid_loss.update(loss.item(), n=batched[0].size(0))\n",
    "                elif mode == 'test':\n",
    "                    out, _ = self.forward_step(batched, calc_loss=False)\n",
    "                preds.append(out.detach().cpu().numpy())\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        preds = self.restore_pred(preds)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def alignment_target_idx(self, y: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "        if self.seq_target_cols is None:\n",
    "            seq_target_cols = []\n",
    "            for col in VERTICAL_TARGET_COLS:\n",
    "                seq_target_cols.extend([f'{col}_{i}' for i in range(60)])\n",
    "            for col in SCALER_TARGET_COLS:\n",
    "                seq_target_cols.append(col)\n",
    "            self.seq_target_cols = seq_target_cols\n",
    "        align_order = [self.seq_target_cols.index(col) for col in self.target_cols]\n",
    "        assert len(y.shape) == 2\n",
    "        y = y[:, align_order]\n",
    "        return y\n",
    "\n",
    "    def convert_target_3dim_to_2dim(self, y: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "        y_v = y[:, :, :len(VERTICAL_TARGET_COLS)]\n",
    "        y_s = y[:, :, len(VERTICAL_TARGET_COLS):]\n",
    "        if type(y) == np.ndarray:\n",
    "        # if isinstance(y, np.ndarray):\n",
    "            y_v = np.transpose(y_v, (0, 2, 1)).reshape(y.shape[0], -1)\n",
    "            y_s = y_s.mean(axis=1)\n",
    "            y = np.concatenate([y_v, y_s], axis=-1)\n",
    "        else:\n",
    "            y_v = y_v.permute(0, 2, 1).reshape(y.size(0), -1)\n",
    "            y_s = y_s.mean(dim=1)\n",
    "            y = torch.cat([y_v, y_s], dim=-1)\n",
    "        y = self.alignment_target_idx(y)\n",
    "        return y\n",
    "\n",
    "    def restore_pred(self, preds: np.ndarray):\n",
    "        return preds * self.config.y_denominators + self.config.y_numerators\n",
    "\n",
    "        ########################################\n",
    "        if eval_only:\n",
    "            self.best_score_dict = pickle.load(open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'rb'))\n",
    "            score, _, preds = self.valid_evaluate(valid_loader, -1, -1, eval_colwise, load_best_weight=True)\n",
    "            self.save_oof_df(preds, self.valid_ids, self.target_cols)\n",
    "            return score, -1\n",
    "        if retrain:\n",
    "            self.best_score_dict = pickle.load(open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'rb'))\n",
    "            self.model.load_state_dict(torch.load(self.output_path / f'{retrain_weight_name}.pth'))\n",
    "\n",
    "        global_step = 0\n",
    "        eval_count = 0 if retrain_eval_count is None else retrain_eval_count + 1\n",
    "        best_score = -np.inf if retrain_best_score is None else retrain_best_score\n",
    "        step_per_epoch = len(train_loader) if self.run_mode != 'hf' else self.config.eval_step\n",
    "        self.init_optimizer_and_scheduler(step_per_epoch=step_per_epoch)\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "            if self.run_mode == 'hf':\n",
    "                del train_loader; gc.collect()\n",
    "                train_loader = self.get_train_loader_from_hf()\n",
    "\n",
    "            iterations = tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            for batched in iterations:\n",
    "                _, loss = self.forward_step(batched, calc_loss=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.train_loss.update(loss.item(), n=batched[0].size(0))\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % self.config.eval_step == 0:\n",
    "                    score, _, preds = self.valid_evaluate(valid_loader, epoch, eval_count)\n",
    "                    if colwise_best_weight:\n",
    "                        torch.save(self.model.state_dict(), self.output_path / f'model{self.save_suffix}_eval{eval_count}.pth')\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preds = preds\n",
    "                        best_epochs = epoch\n",
    "                        torch.save(self.model.state_dict(), self.output_path / f'model{self.save_suffix}_best.pth')\n",
    "                    eval_count += 1\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.7f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.4e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "        pickle.dump(self.best_score_dict, open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'wb'))\n",
    "        if colwise_best_weight:\n",
    "            best_score, _, best_preds = self.valid_evaluate(valid_loader, -1, -1, eval_colwise=True, load_best_weight=True)\n",
    "            self.remove_unuse_weights()\n",
    "\n",
    "        self.save_oof_df(best_preds, self.valid_ids, self.target_cols)\n",
    "        return best_score, best_epochs # 全体スコアが最高の時のEpoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        model: nn.Module,\n",
    "        loss_fn: nn.Module | None = None,\n",
    "        save_suffix: str = '',\n",
    "        logger: loguru._Logger | None = None,\n",
    "        detail_pbar: bool = True\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.run_mode = config.run_mode\n",
    "        self.device = config.device\n",
    "        self.input_path = config.input_path\n",
    "        self.output_path = config.output_path\n",
    "        self.oof_path = config.oof_path\n",
    "\n",
    "        self.target_cols = config.target_cols\n",
    "        self.mul_old_factor = config.mul_old_factor\n",
    "        self.target_min_max = [TARGET_MIN_MAX[col] for col in config.target_cols]\n",
    "        self.factor_dict = get_sub_factor(config.input_path, old=False)\n",
    "        self.old_factor_dict = get_sub_factor(config.input_path, old=True)\n",
    "        self.out_clip = config.out_clip\n",
    "\n",
    "        self.model = model\n",
    "        self.model.to(self.device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "        self.save_suffix = save_suffix\n",
    "        self.logger = logger\n",
    "        self.detail_pbar = detail_pbar\n",
    "\n",
    "        self.hf_ym_list = []\n",
    "        self.valid_ids = None\n",
    "        self.test_ids = None\n",
    "        self.seq_target_cols = None\n",
    "        self.pp_x_cols = [f'state_q0002_{i}' for i in range(12, 27)]\n",
    "        self.pp_y_cols = [f'ptend_q0002_{i}' for i in range(12, 27)]\n",
    "        self.pp_run = len(set(self.target_cols) & set(self.pp_y_cols)) > 0\n",
    "        self.valid_pp_x = None\n",
    "        self.test_pp_x = None\n",
    "        self.best_score_dict = {}\n",
    "\n",
    "    def init_optimizer_and_scheduler(self, step_per_epoch: int):\n",
    "        optimizer = get_optimizer(\n",
    "            self.model,\n",
    "            method=self.config.optimizer_method,\n",
    "            lr=self.config.lr,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            betas=self.config.betas\n",
    "        )\n",
    "        if self.config.scheduler_method == 'linear':\n",
    "            total_steps = self.config.epochs * step_per_epoch\n",
    "            scheduler_args = {\n",
    "                'start_factor': self.config.linear_start_factor,\n",
    "                'end_factor': self.config.linear_end_factor,\n",
    "                'total_iters': total_steps * self.config.linear_end_step_ratio,\n",
    "            }\n",
    "        elif self.config.scheduler_method == 'multistep':\n",
    "            scheduler_args = {\n",
    "                'milestones': self.config.multi_milestones,\n",
    "                'gamma': self.config.multi_gamma,\n",
    "            }\n",
    "        elif self.config.scheduler_method == 'cosine':\n",
    "            T_0 = self.config.cosine_t0_epoch * step_per_epoch\n",
    "            scheduler_args = {\n",
    "                'T_0': T_0,\n",
    "                'T_mult': self.config.cosine_t_mult,\n",
    "                'eta_min': self.config.cosine_min_lr,\n",
    "                'warmup_steps': self.config.cosine_warmup_steps,\n",
    "                'gamma': self.config.cosine_gamma\n",
    "            }\n",
    "        scheduler = get_scheduler(\n",
    "            optimizer,\n",
    "            method=self.config.scheduler_method,\n",
    "            scheduler_args=scheduler_args\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def get_train_loader_from_hf(self, files_per_epoch: int = 5):\n",
    "        npy_path = self.config.input_path / 'huggingface' / 'npy'\n",
    "        if len(self.hf_ym_list) == 0:\n",
    "            npy_files = list(npy_path.glob('X_*.npy'))\n",
    "            self.hf_ym_list = [file.stem.split('_')[1] for file in npy_files]\n",
    "        ym_extract = random.sample(self.hf_ym_list, min(files_per_epoch, len(self.hf_ym_list)))\n",
    "        self.hf_ym_list = [ym for ym in self.hf_ym_list if ym not in ym_extract]\n",
    "        X, y = [], []\n",
    "        for ym in ym_extract:\n",
    "            X.append(np.load(npy_path / f'X_{ym}.npy'))\n",
    "            y.append(np.load(npy_path / f'y_{ym}.npy'))\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        y = np.concatenate(y, axis=0)\n",
    "        train_loader = get_dataloader(self.config, sample_ids=None, X=X, y=y, is_train=True)\n",
    "        del X, y; gc.collect()\n",
    "        return train_loader\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader | None,\n",
    "        valid_loader: DataLoader,\n",
    "        colwise_best_weight: bool = False,\n",
    "        eval_only: bool = False,\n",
    "        eval_colwise: bool = False,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str | None = None,\n",
    "        retrain_best_score: float | None = None,\n",
    "        retrain_eval_count: int | None = None, # 前回の最終eval_countを指定する\n",
    "    ) -> Tuple[float, int]:\n",
    "\n",
    "        if eval_only:\n",
    "            self.best_score_dict = pickle.load(open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'rb'))\n",
    "            score, _, preds = self.valid_evaluate(valid_loader, -1, -1, eval_colwise, load_best_weight=True)\n",
    "            self.save_oof_df(preds, self.valid_ids, self.target_cols)\n",
    "            return score, -1\n",
    "        if retrain:\n",
    "            self.best_score_dict = pickle.load(open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'rb'))\n",
    "            self.model.load_state_dict(torch.load(self.output_path / f'{retrain_weight_name}.pth'))\n",
    "\n",
    "        global_step = 0\n",
    "        eval_count = 0 if retrain_eval_count is None else retrain_eval_count + 1\n",
    "        best_score = -np.inf if retrain_best_score is None else retrain_best_score\n",
    "        step_per_epoch = len(train_loader) if self.run_mode != 'hf' else self.config.eval_step\n",
    "        self.init_optimizer_and_scheduler(step_per_epoch=step_per_epoch)\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "            if self.run_mode == 'hf':\n",
    "                del train_loader; gc.collect()\n",
    "                train_loader = self.get_train_loader_from_hf()\n",
    "\n",
    "            iterations = tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            for batched in iterations:\n",
    "                _, loss = self.forward_step(batched, calc_loss=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.train_loss.update(loss.item(), n=batched[0].size(0))\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % self.config.eval_step == 0:\n",
    "                    score, _, preds = self.valid_evaluate(valid_loader, epoch, eval_count)\n",
    "                    if colwise_best_weight:\n",
    "                        torch.save(self.model.state_dict(), self.output_path / f'model{self.save_suffix}_eval{eval_count}.pth')\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preds = preds\n",
    "                        best_epochs = epoch\n",
    "                        torch.save(self.model.state_dict(), self.output_path / f'model{self.save_suffix}_best.pth')\n",
    "                    eval_count += 1\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.7f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.4e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "        pickle.dump(self.best_score_dict, open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'wb'))\n",
    "        if colwise_best_weight:\n",
    "            best_score, _, best_preds = self.valid_evaluate(valid_loader, -1, -1, eval_colwise=True, load_best_weight=True)\n",
    "            self.remove_unuse_weights()\n",
    "\n",
    "        self.save_oof_df(best_preds, self.valid_ids, self.target_cols)\n",
    "        return best_score, best_epochs # 全体スコアが最高の時のEpoch\n",
    "\n",
    "    def valid_evaluate(\n",
    "        self,\n",
    "        valid_loader: DataLoader,\n",
    "        current_epoch: int,\n",
    "        eval_count: int,\n",
    "        eval_colwise: bool = False,\n",
    "        load_best_weight: bool = False,\n",
    "    ):\n",
    "        if self.valid_ids is None:\n",
    "            self.valid_ids = valid_loader.dataset.sample_ids\n",
    "\n",
    "        if eval_colwise:\n",
    "            preds = self.inference_loop_colwise(valid_loader, 'valid', self.best_score_dict)\n",
    "        else:\n",
    "            preds = self.inference_loop(valid_loader, 'valid', load_best_weight)\n",
    "\n",
    "        labels = valid_loader.dataset.y\n",
    "        if self.config.target_shape == '3dim':\n",
    "            labels = self.convert_target_3dim_to_2dim(labels)\n",
    "        labels = self.restore_pred(labels)\n",
    "\n",
    "        if self.pp_run and self.valid_pp_x is None:\n",
    "            self.load_input_for_postprocess('valid')\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type='valid')\n",
    "        if self.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        eval_idx = [i for i, col in enumerate(self.target_cols) if self.factor_dict[col] != 0]  # factor_dictの値が0のものは自動でR2=1になるようにする\n",
    "        score, indiv_score = evaluate_metric(preds, labels, individual=True, eval_idx=eval_idx)\n",
    "        save_dict = False if load_best_weight else True # 通常の学習ループの時のみbest_score_dictの保存を行う\n",
    "        colwise_score = self.update_best_score(indiv_score, eval_count, save_dict=save_dict)\n",
    "        message = f\"\"\"\n",
    "            [Valid] :\n",
    "                Epoch={current_epoch},\n",
    "                Loss={self.valid_loss.avg:.7f},\n",
    "                Score={score:.5f},\n",
    "                Best Col-Wise Score={colwise_score:.5f}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, colwise_score, preds\n",
    "\n",
    "    def test_predict(self, test_loader: DataLoader, eval_colwise: bool = False) -> pl.DataFrame:\n",
    "        if self.test_ids is None:\n",
    "            self.test_ids = test_loader.dataset.sample_ids\n",
    "\n",
    "        if eval_colwise:\n",
    "            self.best_score_dict = pickle.load(open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'rb'))\n",
    "            preds = self.inference_loop_colwise(test_loader, 'test', self.best_score_dict)\n",
    "        else:\n",
    "            preds = self.inference_loop(test_loader, 'test', load_best_weight=True)\n",
    "\n",
    "        if self.pp_run and self.test_pp_x is None:\n",
    "            self.load_input_for_postprocess('test')\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type='test')\n",
    "        if self.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        pred_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        pred_df = pred_df.with_columns(sample_id = pl.Series(self.test_ids))\n",
    "        return pred_df\n",
    "\n",
    "    def forward_step(self, batched: torch.Tensor, calc_loss: bool = True):\n",
    "        if calc_loss:\n",
    "            x, y = batched\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            out = self.model(x)\n",
    "            if self.config.target_shape == '3dim':\n",
    "                out = self.convert_target_3dim_to_2dim(out)\n",
    "                y = self.convert_target_3dim_to_2dim(y)\n",
    "            loss = self.loss_fn(out, y)\n",
    "            return out, loss\n",
    "        else:\n",
    "            x = batched\n",
    "            x = x.to(self.device)\n",
    "            out = self.model(x)\n",
    "            if self.config.target_shape == '3dim':\n",
    "                out = self.convert_target_3dim_to_2dim(out)\n",
    "            return out, None\n",
    "\n",
    "    def inference_loop(\n",
    "        self,\n",
    "        test_loader: DataLoader,\n",
    "        mode: Literal['valid', 'test'],\n",
    "        load_best_weight: bool = False\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == 'valid':\n",
    "            self.valid_loss.reset()\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(torch.load(self.output_path / f'model{self.save_suffix}_best.pth'))\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = tqdm(test_loader, total=len(test_loader)) if self.detail_pbar else test_loader\n",
    "            for batched in iterations:\n",
    "                if mode == 'valid':\n",
    "                    out, loss = self.forward_step(batched, calc_loss=True)\n",
    "                    self.valid_loss.update(loss.item(), n=batched[0].size(0))\n",
    "                elif mode == 'test':\n",
    "                    out, _ = self.forward_step(batched, calc_loss=False)\n",
    "                preds.append(out.detach().cpu().numpy())\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        preds = self.restore_pred(preds)\n",
    "        return preds\n",
    "\n",
    "    def inference_loop_colwise(\n",
    "        self,\n",
    "        test_loader: DataLoader,\n",
    "        mode: Literal['valid', 'test'],\n",
    "        best_score_dict: Dict[str, Tuple[int, float]],\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == 'valid':\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        use_evals = list(set([eval_count for _, (eval_count, _) in best_score_dict.items()]))\n",
    "        final_preds = np.zeros((len(test_loader.dataset), len(self.target_cols)))\n",
    "        for eval_count in tqdm(use_evals, desc='Inference Col-Wise Weight'):\n",
    "            self.model.load_state_dict(torch.load(self.output_path / f'model{self.save_suffix}_eval{eval_count}.pth'))\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                iterations = tqdm(test_loader, total=len(test_loader)) if self.detail_pbar else test_loader\n",
    "                for batched in iterations:\n",
    "                    if mode == 'valid':\n",
    "                        out, loss = self.forward_step(batched, calc_loss=True)\n",
    "                        self.valid_loss.update(loss.item(), n=batched[0].size(0))\n",
    "                    elif mode == 'test':\n",
    "                        out, _ = self.forward_step(batched, calc_loss=False)\n",
    "                    preds.append(out.detach().cpu().numpy())\n",
    "            preds = np.concatenate(preds, axis=0)\n",
    "            preds = self.restore_pred(preds)\n",
    "\n",
    "            target_cols = [col for col, (count, _) in best_score_dict.items() if count == eval_count]\n",
    "            for col in target_cols:\n",
    "                idx = self.target_cols.index(col)\n",
    "                final_preds[:, idx] = preds[:, idx]\n",
    "        return final_preds\n",
    "\n",
    "    def update_best_score(self, indiv_score: List[float], eval_count: int, save_dict: bool):\n",
    "        for col, score in zip(self.target_cols, indiv_score):\n",
    "            if col not in self.best_score_dict or score > self.best_score_dict[col][1]:\n",
    "                self.best_score_dict[col] = (eval_count, score)\n",
    "        best_colwise_score = (np.sum([score for _, score in self.best_score_dict.values()]) + (368 - len(self.target_cols))) / 368\n",
    "        if save_dict:\n",
    "            pickle.dump(self.best_score_dict, open(self.output_path / f'best_score_dict{self.save_suffix}.pkl', 'wb'))\n",
    "        return best_colwise_score\n",
    "\n",
    "    def remove_unuse_weights(self):\n",
    "        use_eval_counts = set([v[0] for v in self.best_score_dict.values()])\n",
    "        weight_paths = list(self.output_path.glob(f'model{self.save_suffix}_eval*.pth'))\n",
    "        for path in weight_paths:\n",
    "            eval_count = int(path.stem.split('_')[-1].replace('eval', ''))\n",
    "            if eval_count not in use_eval_counts:\n",
    "                path.unlink()\n",
    "\n",
    "    def convert_target_3dim_to_2dim(self, y: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "        y_v = y[:, :, :len(VERTICAL_TARGET_COLS)]\n",
    "        y_s = y[:, :, len(VERTICAL_TARGET_COLS):]\n",
    "        if type(y) == np.ndarray:\n",
    "            y_v = np.transpose(y_v, (0, 2, 1)).reshape(y.shape[0], -1)\n",
    "            y_s = y_s.mean(axis=1)\n",
    "            y = np.concatenate([y_v, y_s], axis=-1)\n",
    "        else:\n",
    "            y_v = y_v.permute(0, 2, 1).reshape(y.size(0), -1)\n",
    "            y_s = y_s.mean(dim=1)\n",
    "            y = torch.cat([y_v, y_s], dim=-1)\n",
    "        y = self.alignment_target_idx(y)\n",
    "        return y\n",
    "\n",
    "    def alignment_target_idx(self, y: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "        if self.seq_target_cols is None:\n",
    "            seq_target_cols = []\n",
    "            for col in VERTICAL_TARGET_COLS:\n",
    "                seq_target_cols.extend([f'{col}_{i}' for i in range(60)])\n",
    "            for col in SCALER_TARGET_COLS:\n",
    "                seq_target_cols.append(col)\n",
    "            self.seq_target_cols = seq_target_cols\n",
    "        align_order = [self.seq_target_cols.index(col) for col in self.target_cols]\n",
    "        assert len(y.shape) == 2\n",
    "        y = y[:, align_order]\n",
    "        return y\n",
    "\n",
    "    def restore_pred(self, preds: np.ndarray):\n",
    "        return preds * self.config.y_denominators + self.config.y_numerators\n",
    "\n",
    "    def clipping_pred(self, preds: np.ndarray):\n",
    "        for i in range(preds.shape[1]):\n",
    "            preds[:, i] = np.clip(\n",
    "                preds[:, i],\n",
    "                self.target_min_max[i][0],\n",
    "                self.target_min_max[i][1]\n",
    "            )\n",
    "        return preds\n",
    "\n",
    "    def postprocess(self, preds: np.ndarray, run_type: Literal['valid', 'test']):\n",
    "        pp_x = self.valid_pp_x if run_type == 'valid' else self.test_pp_x\n",
    "        for y_col, x_col in zip(self.pp_y_cols, self.pp_x_cols):\n",
    "            if y_col in self.target_cols:\n",
    "                idx = self.target_cols.index(y_col)\n",
    "                old_factor = self.old_factor_dict[y_col] if self.mul_old_factor else 1\n",
    "                preds[:, idx] = (-1 * pp_x[x_col].to_numpy() / 1200) * old_factor\n",
    "        return preds\n",
    "\n",
    "    def load_input_for_postprocess(self, data_type: Literal['valid', 'test']):\n",
    "        if data_type == 'valid':\n",
    "            self.valid_pp_x = (\n",
    "                pl.scan_parquet(self.input_path / 'train_pp.parquet')\n",
    "                .select(['sample_id'] + self.pp_x_cols)\n",
    "                .filter(pl.col('sample_id').is_in(self.valid_ids))\n",
    "                .collect()\n",
    "            )\n",
    "            id_df = pl.DataFrame({'sample_id': self.valid_ids})\n",
    "            self.valid_pp_x = id_df.join(self.valid_pp_x, on='sample_id', how='left')\n",
    "\n",
    "        elif data_type == 'test':\n",
    "            self.test_pp_x = pl.read_parquet(\n",
    "                Config.input_path / 'test_pp.parquet',\n",
    "                columns=['sample_id'] + self.pp_x_cols\n",
    "            )\n",
    "            id_df = pl.DataFrame({'sample_id': self.test_ids})\n",
    "            self.test_pp_x = id_df.join(self.test_pp_x, on='sample_id', how='left')\n",
    "\n",
    "    def save_oof_df(self, preds: np.ndarray, sample_ids: np.ndarray, target_cols: List[str]):\n",
    "        oof_df = pl.DataFrame(preds, schema=target_cols)\n",
    "        oof_df = oof_df.with_columns(sample_id = pl.Series(sample_ids))\n",
    "        oof_df.write_parquet(self.oof_path / f'oof{self.save_suffix}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "save_suffix = ''\n",
    "trainer = Trainer(config, model, loss_fn, save_suffix=save_suffix, logger=logger)\n",
    "best_score, best_epoch = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    colwise_best_weight=True,\n",
    "    # eval_only=True,\n",
    "    # eval_colwise=True,\n",
    "    retrain=True,\n",
    "    retrain_weight_name='model_eval239',\n",
    "    retrain_best_score=0.78355,\n",
    "    retrain_eval_count=239, # 前回の最終eval_countを指定する\n",
    ")\n",
    "logger.info(f'At Epoch={best_epoch}, Best Score={best_score:.4f}')\n",
    "pred_df = trainer.test_predict(test_loader, eval_colwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
