{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import seed_everything, get_logger, get_config, TimeUtil\n",
    "from src.utils.competition_utils import clipping_input\n",
    "from src.data import DataProvider, FeatureEngineering, Preprocessor, HFPreprocessor\n",
    "from src.train import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = '146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-12 23:37:46\u001b[0m | \u001b[1mINFO ] exp: 146 | run_mode=hf, multi_task=False, loss_type=mae\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path('../config'))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f'exp: {exp} | run_mode={config.run_mode}, multi_task={config.multi_task}, loss_type={config.loss_type}')\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.run_mode = 'hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] start [0.5GB(2.0%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] done [109.1GB(45.0%)(+108.550GB)] 112.9162 s\n",
      "[Feature Engineering...] start [109.1GB(45.0%)]\n",
      "[Feature Engineering...] done [133.5GB(33.6%)(+24.372GB)] 43.2264 s\n",
      "[Scaling and Clipping Features...] start [133.5GB(33.6%)]\n",
      "[Scaling and Clipping Features...] done [108.4GB(22.6%)(-25.101GB)] 94.7392 s\n",
      "[Converting to arrays for NN...] start [108.4GB(22.6%)]\n",
      "[Converting to arrays for NN...] done [148.0GB(46.0%)(+39.667GB)] 314.0020 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Data Loading...'):\n",
    "    dpr = DataProvider(config)\n",
    "    train_df, test_df = dpr.load_data()\n",
    "\n",
    "with TimeUtil.timer('Feature Engineering...'):\n",
    "    fer = FeatureEngineering(config)\n",
    "    train_df = fer.feature_engineering(train_df)\n",
    "    test_df = fer.feature_engineering(test_df)\n",
    "\n",
    "with TimeUtil.timer('Scaling and Clipping Features...'):\n",
    "    ppr = Preprocessor(config)\n",
    "    train_df, test_df = ppr.scaling(train_df, test_df)\n",
    "    input_cols, target_cols = ppr.input_cols, ppr.target_cols\n",
    "    if config.task_type == 'grid_pred':\n",
    "        train_df = train_df.drop(target_cols)\n",
    "\n",
    "    valid_df = train_df.filter(pl.col('fold') == 0)\n",
    "    train_df = train_df.filter(pl.col('fold') != 0)\n",
    "    valid_df, input_clip_dict = clipping_input(train_df, valid_df, input_cols)\n",
    "    test_df, _ = clipping_input(None, test_df, input_cols, input_clip_dict)\n",
    "    pickle.dump(input_clip_dict, open(config.output_path / 'input_clip_dict.pkl', 'wb'))\n",
    "\n",
    "with TimeUtil.timer('Converting to arrays for NN...'):\n",
    "    array_data = ppr.convert_numpy_array(train_df, valid_df, test_df)\n",
    "    del train_df, valid_df, test_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF Data Preprocessing...] start [48.8GB(6.5%)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fbc810d1c9402cb02914c81bb959d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF Data Preprocessing...] done [61.8GB(6.5%)(+13.077GB)] 555.8223 s\n"
     ]
    }
   ],
   "source": [
    "if config.run_mode == 'hf':\n",
    "    with TimeUtil.timer('HF Data Preprocessing...'):\n",
    "        del array_data['train_ids'], array_data['X_train'], array_data['y_train']\n",
    "        gc.collect()\n",
    "\n",
    "        hf_ppr = HFPreprocessor(config)\n",
    "        hf_ppr.shrink_file_size()\n",
    "        hf_ppr.convert_numpy_array(unlink_parquet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TimeUtil.timer('Creating Torch DataLoader...'):\n",
    "    if config.run_mode == 'hf':\n",
    "        train_loader = get_dataloader(\n",
    "            config,\n",
    "            from_hdf5=False,\n",
    "            is_train=True\n",
    "        )\n",
    "    else:\n",
    "        train_loader = get_dataloader(\n",
    "            config,\n",
    "            array_data['train_ids'],\n",
    "            array_data['X_train'],\n",
    "            array_data['y_train'],\n",
    "            is_train=True\n",
    "        )\n",
    "    valid_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['valid_ids'],\n",
    "        array_data['X_valid'],\n",
    "        array_data['y_valid'],\n",
    "        is_train=False\n",
    "    )\n",
    "    test_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['test_ids'],\n",
    "        array_data['X_test'],\n",
    "        is_train=False\n",
    "    )\n",
    "    del array_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/utils/model_ema.py\n",
    "class ModelEmaV3(nn.Module):\n",
    "    \"\"\"Model Exponential Moving Average V3\n",
    "\n",
    "    Keep a moving average of everything in the model state_dict (parameters and buffers).\n",
    "    V3 of this module leverages for_each and in-place operations for faster performance.\n",
    "\n",
    "    Decay warmup based on code by @crowsonkb, her comments:\n",
    "      If inv_gamma=1 and power=1, implements a simple average. inv_gamma=1, power=2/3 are\n",
    "      good values for models you plan to train for a million or more steps (reaches decay\n",
    "      factor 0.999 at 31.6K steps, 0.9999 at 1M steps), inv_gamma=1, power=3/4 for models\n",
    "      you plan to train for less (reaches decay factor 0.999 at 10K steps, 0.9999 at\n",
    "      215.4k steps).\n",
    "\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "\n",
    "    To keep EMA from using GPU resources, set device='cpu'. This will save a bit of memory but\n",
    "    disable validation of the EMA weights. Validation will have to be done manually in a separate\n",
    "    process, or after the training stops converging.\n",
    "\n",
    "    This class is sensitive where it is initialized in the sequence of model init,\n",
    "    GPU assignment and distributed training wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        decay: float = 0.9999,\n",
    "        min_decay: float = 0.0,\n",
    "        update_after_step: int = 0,\n",
    "        use_warmup: bool = False,\n",
    "        warmup_gamma: float = 1.0,\n",
    "        warmup_power: float = 2 / 3,\n",
    "        device: torch.device | None = None,\n",
    "        foreach: bool = True,\n",
    "        exclude_buffers: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.min_decay = min_decay\n",
    "        self.update_after_step = update_after_step\n",
    "        self.use_warmup = use_warmup\n",
    "        self.warmup_gamma = warmup_gamma\n",
    "        self.warmup_power = warmup_power\n",
    "        self.foreach = foreach\n",
    "        self.device = (\n",
    "            \"cuda:0\" if device == \"cuda\" else device\n",
    "        )  # perform ema on different device from model if set\n",
    "        self.exclude_buffers = exclude_buffers\n",
    "        if self.device is not None and device != next(model.parameters()).device:\n",
    "            self.foreach = False  # cannot use foreach methods with different devices\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def get_decay(self, step: int | None = None) -> float:\n",
    "        \"\"\"\n",
    "        Compute the decay factor for the exponential moving average.\n",
    "        \"\"\"\n",
    "        if step is None:\n",
    "            return self.decay\n",
    "\n",
    "        step = max(0, step - self.update_after_step - 1)\n",
    "        if step <= 0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.use_warmup:\n",
    "            decay = 1 - (1 + step / self.warmup_gamma) ** -self.warmup_power\n",
    "            decay = max(min(decay, self.decay), self.min_decay)\n",
    "        else:\n",
    "            decay = self.decay\n",
    "\n",
    "        return decay\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model, step: int | None = None):\n",
    "        decay = self.get_decay(step)\n",
    "        if self.exclude_buffers:\n",
    "            self.apply_update_no_buffers_(model, decay)\n",
    "        else:\n",
    "            self.apply_update_(model, decay)\n",
    "\n",
    "    def apply_update_(self, model, decay: float):\n",
    "        # interpolate parameters and buffers\n",
    "        if self.foreach:\n",
    "            ema_lerp_values = []\n",
    "            model_lerp_values = []\n",
    "            for ema_v, model_v in zip(\n",
    "                self.module.state_dict().values(), model.state_dict().values()\n",
    "            ):\n",
    "                if ema_v.is_floating_point():\n",
    "                    ema_lerp_values.append(ema_v)\n",
    "                    model_lerp_values.append(model_v)\n",
    "                else:\n",
    "                    ema_v.copy_(model_v)\n",
    "\n",
    "            if hasattr(torch, \"_foreach_lerp_\"):\n",
    "                torch._foreach_lerp_(ema_lerp_values, model_lerp_values, weight=1.0 - decay)\n",
    "            else:\n",
    "                torch._foreach_mul_(ema_lerp_values, scalar=decay)\n",
    "                torch._foreach_add_(ema_lerp_values, model_lerp_values, alpha=1.0 - decay)\n",
    "        else:\n",
    "            for ema_v, model_v in zip(\n",
    "                self.module.state_dict().values(), model.state_dict().values()\n",
    "            ):\n",
    "                if ema_v.is_floating_point():\n",
    "                    ema_v.lerp_(model_v, weight=1.0 - decay)\n",
    "                else:\n",
    "                    ema_v.copy_(model_v)\n",
    "\n",
    "    def apply_update_no_buffers_(self, model, decay: float):\n",
    "        # interpolate parameters, copy buffers\n",
    "        ema_params = tuple(self.module.parameters())\n",
    "        model_params = tuple(model.parameters())\n",
    "        if self.foreach:\n",
    "            if hasattr(torch, \"_foreach_lerp_\"):\n",
    "                torch._foreach_lerp_(ema_params, model_params, weight=1.0 - decay)\n",
    "            else:\n",
    "                torch._foreach_mul_(ema_params, scalar=decay)\n",
    "                torch._foreach_add_(ema_params, model_params, alpha=1 - decay)\n",
    "        else:\n",
    "            for ema_p, model_p in zip(ema_params, model_params):\n",
    "                ema_p.lerp_(model_p, weight=1.0 - decay)\n",
    "\n",
    "        for ema_b, model_b in zip(self.module.buffers(), model.buffers()):\n",
    "            ema_b.copy_(model_b.to(device=self.device))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def set(self, model):\n",
    "        for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "            ema_v.copy_(model_v.to(device=self.device))\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.module(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Literal, Tuple\n",
    "\n",
    "import loguru\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.train import ComponentFactory\n",
    "from src.train.train_utils import AverageMeter\n",
    "from src.utils import clean_message\n",
    "from src.utils.competition_utils import evaluate_metric, get_io_columns, get_sub_factor\n",
    "from src.utils.constant import (\n",
    "    PP_TARGET_COLS,\n",
    "    SCALER_TARGET_COLS,\n",
    "    TARGET_MIN_MAX,\n",
    "    VERTICAL_TARGET_COLS,\n",
    ")\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, save_suffix: str = \"\"):\n",
    "        self.config = config\n",
    "        self.eval_step = config.eval_step[config.run_mode]\n",
    "        self.logger = logger\n",
    "        self.save_suffix = save_suffix\n",
    "        self.detail_pbar = True\n",
    "\n",
    "        self.model = ComponentFactory.get_model(config)\n",
    "        self.model = self.model.to(config.device)\n",
    "        n_device = torch.cuda.device_count()\n",
    "        if n_device > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        if config.ema:\n",
    "            self.model_ema = None\n",
    "\n",
    "        self.loss_fn = ComponentFactory.get_loss(config)\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "\n",
    "        _, self.target_cols = get_io_columns(config)\n",
    "        self.model_target_cols = self.get_model_target_cols()\n",
    "        self.factor_dict = get_sub_factor(config.input_path, old=False)\n",
    "        self.old_factor_dict = get_sub_factor(config.input_path, old=True)\n",
    "\n",
    "        self.y_numerators = np.load(\n",
    "            config.output_path / f\"y_numerators_{config.target_scale_method}.npy\"\n",
    "        )\n",
    "        self.y_denominators = np.load(\n",
    "            config.output_path / f\"y_denominators_{config.target_scale_method}.npy\"\n",
    "        )\n",
    "        self.target_min_max = [TARGET_MIN_MAX[col] for col in self.target_cols]\n",
    "\n",
    "        self.valid_ids = None\n",
    "        self.test_ids = None\n",
    "        self.valid_pp_df = None\n",
    "        self.test_pp_df = None\n",
    "        self.pp_run = True\n",
    "        self.pp_y_cols = PP_TARGET_COLS\n",
    "        self.pp_x_cols = [col.replace(\"ptend\", \"state\") for col in self.pp_y_cols]\n",
    "\n",
    "        self.best_score_dict = defaultdict(lambda: (-1, -np.inf))\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        colwise_mode: bool = True,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str = \"\",\n",
    "        retrain_best_score: float = -np.inf,\n",
    "        eval_only: bool = False,\n",
    "    ):\n",
    "        if eval_only:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            eval_method = \"colwise\" if colwise_mode else \"single\"\n",
    "            score, cw_score, preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=eval_method\n",
    "            )\n",
    "            self.save_oof_df(self.valid_ids, preds)\n",
    "            return score, cw_score, -1\n",
    "\n",
    "        self.optimizer = ComponentFactory.get_optimizer(self.config, self.model)\n",
    "        steps_per_epoch = len(train_loader) if self.config.run_mode != 'hf' else self.config.eval_step[self.config.run_mode]\n",
    "        self.scheduler = ComponentFactory.get_scheduler(\n",
    "            self.config, self.optimizer, steps_per_epoch=steps_per_epoch\n",
    "        )\n",
    "        global_step = 0\n",
    "        eval_count = 0\n",
    "        best_score = -np.inf\n",
    "\n",
    "        if retrain:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\")\n",
    "            )\n",
    "            weight_numbers = [\n",
    "                int(file.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "                for file in list(self.config.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "            ]\n",
    "            eval_count = sorted(weight_numbers)[-1] + 1\n",
    "            best_score = retrain_best_score\n",
    "\n",
    "        # 学習ループの開始\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            iterations = (\n",
    "                tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            )\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(data, calc_loss=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % self.eval_step == 0:\n",
    "                    score, _, preds, update_num = self.valid_evaluate(\n",
    "                        valid_loader,\n",
    "                        current_epoch=epoch,\n",
    "                        eval_count=eval_count,\n",
    "                        eval_method=\"single\",\n",
    "                    )\n",
    "                    if colwise_mode and update_num > 0:\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            self.config.output_path\n",
    "                            / f\"model{self.save_suffix}_eval{eval_count}.pth\",\n",
    "                        )\n",
    "\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preds = preds\n",
    "                        best_epochs = epoch\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            self.config.output_path / f\"model{self.save_suffix}_best.pth\",\n",
    "                        )\n",
    "\n",
    "                    eval_count += 1\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.5f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.5e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "            if self.config.run_mode == 'hf':\n",
    "                train_loader = self.update_train_loader(train_loader)\n",
    "\n",
    "        if colwise_mode:\n",
    "            self.remove_unuse_weights()\n",
    "            best_score, best_cw_score, best_preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=\"colwise\"\n",
    "            )\n",
    "\n",
    "        self.save_oof_df(self.valid_ids, best_preds)\n",
    "        return best_score, best_cw_score, best_epochs\n",
    "\n",
    "    def valid_evaluate(\n",
    "        self,\n",
    "        valid_loader: DataLoader,\n",
    "        current_epoch: int,\n",
    "        eval_count: int,\n",
    "        eval_method: Literal[\"single\", \"colwise\"] = \"single\",\n",
    "    ):\n",
    "        if self.valid_ids is None:\n",
    "            self.valid_ids = valid_loader.dataset.ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            load_best_weight = True if eval_count == -1 else False\n",
    "            preds = self.inference_loop(\n",
    "                valid_loader, mode=\"valid\", load_best_weight=load_best_weight\n",
    "            )\n",
    "        elif eval_method == \"colwise\":\n",
    "            preds = self.inference_loop_colwise(valid_loader, \"valid\", self.best_score_dict)\n",
    "\n",
    "        labels = valid_loader.dataset.y\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            labels = self.convert_target_3dim_to_2dim(labels)\n",
    "        preds = self.restore_pred(preds)\n",
    "        labels = self.restore_pred(labels)\n",
    "\n",
    "        if self.pp_run and self.valid_pp_df is None:\n",
    "            self.load_postprocess_input(\"valid\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"valid\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        eval_idx = [\n",
    "            i for i, col in enumerate(self.target_cols) if self.factor_dict[col] != 0\n",
    "        ]  # factor_dictの値が0のものは自動でR2=1になるようにする\n",
    "        score, indiv_scores = evaluate_metric(preds, labels, eval_idx=eval_idx)\n",
    "        cw_score, update_num = self.update_best_score(indiv_scores, eval_count)\n",
    "\n",
    "        message = f\"\"\"\n",
    "            [Valid] :\n",
    "                Epoch={current_epoch},\n",
    "                Loss={self.valid_loss.avg:.5f},\n",
    "                Score={score:.5f},\n",
    "                Best Col-Wise Score={cw_score:.5f}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, cw_score, preds, update_num\n",
    "\n",
    "    def test_predict(\n",
    "        self, test_loader: DataLoader, eval_method: Literal[\"single\", \"colwise\"] = \"single\"\n",
    "    ):\n",
    "        if self.test_ids is None:\n",
    "            self.test_ids = test_loader.dataset.ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            preds = self.inference_loop(test_loader, mode=\"test\", load_best_weight=True)\n",
    "        elif eval_method == \"colwise\":\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            preds = self.inference_loop_colwise(test_loader, \"test\", self.best_score_dict)\n",
    "\n",
    "        preds = self.restore_pred(preds)\n",
    "        if self.pp_run and self.test_pp_df is None:\n",
    "            self.load_postprocess_input(\"test\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"test\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        pred_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        pred_df = pred_df.with_columns(sample_id=pl.Series(self.test_ids))\n",
    "        return pred_df\n",
    "\n",
    "    def forward_step(self, data: torch.Tensor, calc_loss: bool = True):\n",
    "        if calc_loss:\n",
    "            x, y = data\n",
    "            x, y = x.to(self.config.device), y.to(self.config.device)\n",
    "            out = self.model(x)\n",
    "            loss = self.loss_fn(out, y)\n",
    "        else:\n",
    "            x = data[0]\n",
    "            x = x.to(self.config.device)\n",
    "            out = self.model(x)\n",
    "            loss = None\n",
    "\n",
    "        if self.config.multi_task:\n",
    "            out = out[:, :, :self.config.out_dim]\n",
    "\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            out = self.convert_target_3dim_to_2dim(out)\n",
    "        return out, loss\n",
    "\n",
    "    def inference_loop(\n",
    "        self,\n",
    "        eval_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        load_best_weight: bool = False,\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\")\n",
    "            )\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = (\n",
    "                tqdm(eval_loader, total=len(eval_loader)) if self.detail_pbar else eval_loader\n",
    "            )\n",
    "            for data in iterations:\n",
    "                if mode == \"valid\":\n",
    "                    out, loss = self.forward_step(data, calc_loss=True)\n",
    "                    self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                elif mode == \"test\":\n",
    "                    out, _ = self.forward_step(data, calc_loss=False)\n",
    "                preds.append(out.detach().cpu().numpy())\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        return preds\n",
    "\n",
    "    def inference_loop_colwise(\n",
    "        self,\n",
    "        test_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        best_score_dict: dict[str, tuple[int, float]],\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        selected_counts = list(set([eval_count for eval_count, _ in best_score_dict.values()]))\n",
    "        all_preds = np.zeros((len(test_loader.dataset), len(self.target_cols)))\n",
    "        for eval_count in tqdm(selected_counts):\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    self.config.output_path / f\"model{self.save_suffix}_eval{eval_count}.pth\"\n",
    "                )\n",
    "            )\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                iterations = (\n",
    "                    tqdm(test_loader, total=len(test_loader)) if self.detail_pbar else test_loader\n",
    "                )\n",
    "                for data in iterations:\n",
    "                    if mode == \"valid\":\n",
    "                        out, loss = self.forward_step(data, calc_loss=True)\n",
    "                        self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                    elif mode == \"test\":\n",
    "                        out, _ = self.forward_step(data, calc_loss=False)\n",
    "                    preds.append(out.detach().cpu().numpy())\n",
    "            preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "            target_cols = [\n",
    "                col for col, (count, _) in best_score_dict.items() if count == eval_count\n",
    "            ]\n",
    "            for col in target_cols:\n",
    "                idx = self.target_cols.index(col)\n",
    "                all_preds[:, idx] = preds[:, idx]\n",
    "        return all_preds\n",
    "\n",
    "    def update_train_loader(self, train_loader: DataLoader):\n",
    "        train_dataset = train_loader.dataset\n",
    "        train_dataset.update()\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.train_batch,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def update_best_score(self, indiv_scores: list[float], eval_count: int):\n",
    "        update_num = 0\n",
    "        for col, score in zip(self.target_cols, indiv_scores):\n",
    "            if score > self.best_score_dict[col][1] and eval_count != -1:\n",
    "                self.best_score_dict[col] = (eval_count, score)\n",
    "                update_num += 1\n",
    "\n",
    "        best_cw_score = (\n",
    "            np.sum([score for _, score in self.best_score_dict.values()])\n",
    "            + (368 - len(self.target_cols))\n",
    "        ) / 368\n",
    "        if update_num > 0 and eval_count != -1:\n",
    "            pickle.dump(\n",
    "                dict(self.best_score_dict),\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"wb\"),\n",
    "            )\n",
    "        return best_cw_score, update_num\n",
    "\n",
    "    def remove_unuse_weights(self):\n",
    "        selected_counts = set([v[0] for v in self.best_score_dict.values()])\n",
    "        weight_paths = list(self.config.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "        for path in weight_paths:\n",
    "            eval_count = int(path.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "            if eval_count not in selected_counts:\n",
    "                path.unlink()\n",
    "\n",
    "    def convert_target_3dim_to_2dim(\n",
    "        self, y: np.ndarray | torch.Tensor\n",
    "    ) -> np.ndarray | torch.Tensor:\n",
    "        y_v = y[:, :, : len(VERTICAL_TARGET_COLS)]\n",
    "        y_s = y[:, :, len(VERTICAL_TARGET_COLS) :]\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y_v = np.transpose(y_v, (0, 2, 1)).reshape(y.shape[0], -1)\n",
    "            y_s = y_s.mean(axis=1)\n",
    "            y = np.concatenate([y_v, y_s], axis=-1)\n",
    "        elif isinstance(y, torch.Tensor):\n",
    "            y_v = y_v.permute(0, 2, 1).reshape(y.size(0), -1)\n",
    "            y_s = y_s.mean(dim=1)\n",
    "            y = torch.cat([y_v, y_s], dim=-1)\n",
    "        y = self.alignment_target_idx(y)\n",
    "        return y\n",
    "\n",
    "    def alignment_target_idx(self, y: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "        align_order = [self.model_target_cols.index(col) for col in self.target_cols]\n",
    "        assert len(y.shape) == 2\n",
    "        y = y[:, align_order]\n",
    "        return y\n",
    "\n",
    "    def get_model_target_cols(self):\n",
    "        model_target_cols = []\n",
    "        for col in VERTICAL_TARGET_COLS:\n",
    "            model_target_cols.extend([f\"{col}_{i}\" for i in range(60)])\n",
    "        for col in SCALER_TARGET_COLS:\n",
    "            model_target_cols.append(col)\n",
    "        return model_target_cols\n",
    "\n",
    "    def restore_pred(self, preds: np.ndarray):\n",
    "        return preds * self.y_denominators + self.y_numerators\n",
    "\n",
    "    def clipping_pred(self, preds: np.ndarray):\n",
    "        for i in range(preds.shape[1]):\n",
    "            preds[:, i] = np.clip(preds[:, i], self.target_min_max[i][0], self.target_min_max[i][1])\n",
    "        return preds\n",
    "\n",
    "    def save_oof_df(self, sample_ids: np.ndarray, preds: np.ndarray):\n",
    "        oof_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        oof_df = oof_df.with_columns(sample_id=pl.Series(sample_ids))\n",
    "        oof_df.write_parquet(self.config.oof_path / f\"oof{self.save_suffix}.parquet\")\n",
    "\n",
    "    def postprocess(self, preds: np.ndarray, run_type: Literal[\"valid\", \"test\"]):\n",
    "        pp_x = self.valid_pp_df if run_type == \"valid\" else self.test_pp_df\n",
    "        for x_col, y_col in zip(self.pp_x_cols, self.pp_y_cols):\n",
    "            if y_col in self.target_cols:\n",
    "                idx = self.target_cols.index(y_col)\n",
    "                old_factor = self.old_factor_dict[y_col] if self.config.mul_old_factor else 1\n",
    "                preds[:, idx] = (-1 * pp_x[x_col].to_numpy() / 1200) * old_factor\n",
    "        return preds\n",
    "\n",
    "    def load_postprocess_input(self, data_type: Literal[\"valid\", \"test\"]):\n",
    "        if data_type == \"valid\":\n",
    "            valid_path = (\n",
    "                self.config.input_path / \"18_shrinked.parquet\"\n",
    "                if self.config.shared_valid\n",
    "                else self.config.input_path / \"train_shrinked.parquet\"\n",
    "            )\n",
    "            self.valid_pp_df = (\n",
    "                pl.scan_parquet(valid_path)\n",
    "                .select([\"sample_id\"] + self.pp_x_cols)\n",
    "                .filter(pl.col(\"sample_id\").is_in(self.valid_ids))\n",
    "                .collect()\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.valid_ids})\n",
    "            self.valid_pp_df = id_df.join(self.valid_pp_df, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "        elif data_type == \"test\":\n",
    "            self.test_pp_df = pl.read_parquet(\n",
    "                self.config.input_path / \"test_shrinked.parquet\",\n",
    "                columns=[\"sample_id\"] + self.pp_x_cols,\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.test_ids})\n",
    "            self.test_pp_df = id_df.join(self.test_pp_df, on=\"sample_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.eval_step['dev'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd489d19bf342db99bdd2c879919c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4c7bfa023045d7ba7039d4d01a87ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1582 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oof_df = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    colwise_mode=True,\n",
    "    # eval_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32326/383357159.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc369fc74294b2a81a589c43bd4655b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = trainer.test_predict(test_loader, eval_method=\"colwise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "import loguru\n",
    "from typing import Literal\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from src.utils.competition_utils import get_sub_factor, get_io_columns\n",
    "\n",
    "class PostProcess:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, additional: bool = True):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.additional = additional\n",
    "\n",
    "        _, self.target_cols = get_io_columns(config)\n",
    "        self.old_factor_dict = get_sub_factor(config.input_path, old=True)\n",
    "        self.sub_cols = pl.read_parquet(config.input_path / 'sample_submission.parquet', n_rows=1).columns\n",
    "\n",
    "        self.pp_x_cols = [f'state_q0002_{i}' for i in range(12, 27)]\n",
    "        self.pp_y_cols = [f'ptend_q0002_{i}' for i in range(12, 27)]\n",
    "        self.valid_pp_df = pl.read_parquet(\n",
    "            config.input_path / '18_shrinked.parquet',\n",
    "            columns=['sample_id'] + self.pp_x_cols\n",
    "        )\n",
    "        self.test_pp_df = pl.read_parquet(\n",
    "            config.input_path / 'test_shrinked.parquet',\n",
    "            columns=['sample_id'] + self.pp_x_cols\n",
    "        )\n",
    "\n",
    "        add_pp_y_cols = (\n",
    "            [f'ptend_q0002_{i}' for i in range(60)] +\n",
    "            [f'ptend_q0003_{i}' for i in range(60)]\n",
    "        )\n",
    "        self.add_pp_y_cols = [col for col in add_pp_y_cols if col in self.target_cols]\n",
    "        self.add_pp_x_cols = [col.replace('ptend', 'state') for col in self.add_pp_y_cols]\n",
    "        self.add_valid_pp_df = pl.read_parquet(\n",
    "            config.input_path / '18_shrinked.parquet',\n",
    "            columns=self.sub_cols + self.add_pp_x_cols\n",
    "        )\n",
    "        self.add_test_pp_df = pl.read_parquet(\n",
    "            config.input_path / 'test_shrinked.parquet',\n",
    "            columns=['sample_id'] + self.add_pp_x_cols\n",
    "        )\n",
    "        self.th_dict = None\n",
    "\n",
    "    def postprocess(self, oof_df: pl.DataFrame, sub_df: pl.DataFrame):\n",
    "        oof_df = self.complement_columns(oof_df)\n",
    "        oof_df = self.reverse_sub_factor(oof_df)\n",
    "        oof_df = self.replace_postprocess(oof_df, 'oof')\n",
    "\n",
    "        sub_df = self.complement_columns(sub_df)\n",
    "        sub_df = self.reverse_sub_factor(sub_df)\n",
    "        sub_df = self.replace_postprocess(sub_df, 'sub')\n",
    "\n",
    "        if self.additional:\n",
    "            oof_df = self.additional_postprocess(oof_df, 'oof')\n",
    "            sub_df = self.additional_postprocess(sub_df, 'sub')\n",
    "\n",
    "        oof_df = self.create_oof_df(oof_df)\n",
    "        sub_df = self.create_sub_df(sub_df)\n",
    "        return oof_df, sub_df\n",
    "\n",
    "    def complement_columns(self, pred_df: pl.DataFrame):\n",
    "        lack_cols = list(set(self.sub_cols) - set(pred_df.columns))\n",
    "        for col in lack_cols:\n",
    "            pred_df = pred_df.with_columns([pl.lit(0).alias(col)])\n",
    "        return pred_df\n",
    "\n",
    "    def reverse_sub_factor(self, pred_df: pl.DataFrame):\n",
    "        if self.config.mul_old_factor:\n",
    "            exprs = []\n",
    "            for col in self.target_cols:\n",
    "                if self.old_factor_dict[col] != 0:\n",
    "                    exprs.append((pl.col(col) / self.old_factor_dict[col]).alias(col))\n",
    "\n",
    "            pred_df = pred_df.with_columns(exprs)\n",
    "        return pred_df\n",
    "\n",
    "    def replace_postprocess(self, pred_df: pl.DataFrame, pred_type: Literal['oof', 'sub']):\n",
    "        pp_df = self.valid_pp_df if pred_type == 'oof' else self.test_pp_df\n",
    "        pred_df = pred_df.join(pp_df, on=['sample_id'], how='left')\n",
    "\n",
    "        exprs = []\n",
    "        for x_col, y_col in zip(self.pp_x_cols, self.pp_y_cols):\n",
    "            exprs.append((-1 * pl.col(x_col) / 1200).alias(y_col))\n",
    "        pred_df = pred_df.with_columns(exprs)\n",
    "        pred_df = pred_df.drop(self.pp_x_cols)\n",
    "        return pred_df\n",
    "\n",
    "    def additional_postprocess(self, pred_df: pl.DataFrame, pred_type: Literal['oof', 'sub']):\n",
    "        pp_df = self.add_valid_pp_df if pred_type == 'oof' else self.add_test_pp_df\n",
    "        pred_df = pred_df.join(pp_df, on=['sample_id'], how='left', suffix='_gt')\n",
    "        exprs = []\n",
    "        for x_col, y_col in zip(self.add_pp_x_cols, self.add_pp_y_cols):\n",
    "            exprs.append((pl.col(x_col) + pl.col(y_col) * 1200).alias(f'{x_col}_next'))\n",
    "        pred_df = pred_df.with_columns(exprs)\n",
    "\n",
    "        if pred_type == 'oof':\n",
    "            self.tuning_threshold(pred_df)\n",
    "\n",
    "        assert self.th_dict is not None # oofから実行する必要がある\n",
    "        exprs = []\n",
    "        for y_col, (best_th, _) in self.th_dict.items():\n",
    "            x_col = y_col.replace('ptend', 'state')\n",
    "            exprs.append(\n",
    "                pl.when(pl.col(f'{x_col}_next') < best_th)\n",
    "                .then(-1 * pl.col(x_col) / 1200)\n",
    "                .otherwise(pl.col(y_col))\n",
    "                .alias(y_col)\n",
    "            )\n",
    "        pred_df = pred_df.with_columns(exprs)\n",
    "\n",
    "        if pred_type == 'oof':\n",
    "            scores = []\n",
    "            for col in self.target_cols:\n",
    "                score = r2_score(pred_df[f'{col}_gt'].to_numpy(), pred_df[col].to_numpy())\n",
    "                scores.append(score)\n",
    "            total_score = (np.sum(scores) + (368 - len(scores))) / 368\n",
    "            self.logger.info(f'After Additional Postprocess: {total_score:.5f}')\n",
    "\n",
    "        drop_cols = (\n",
    "            self.add_pp_x_cols +\n",
    "            [f'{col}_next' for col in self.add_pp_x_cols] +\n",
    "            [col for col in pred_df.columns if '_gt' in col]\n",
    "        )\n",
    "        pred_df = pred_df.drop(drop_cols)\n",
    "        return pred_df\n",
    "\n",
    "    def tuning_threshold(self, pred_df: pl.DataFrame):\n",
    "        iterations = tqdm(zip(self.add_pp_x_cols, self.add_pp_y_cols), total=len(self.add_pp_x_cols))\n",
    "        for x_col, y_col in iterations:\n",
    "            best_score = r2_score(pred_df[f'{y_col}_gt'].to_numpy(), pred_df[y_col].to_numpy())\n",
    "            best_th = None\n",
    "            for th_base in [0, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5]:\n",
    "                for corr in range(1, 10):\n",
    "                    if th_base == 0 and corr >= 2:\n",
    "                        break\n",
    "\n",
    "                    th = th_base * corr\n",
    "                    preds = pred_df.select(\n",
    "                        pl.when(pl.col(f'{x_col}_next') < th)\n",
    "                        .then(-1 * pl.col(x_col) / 1200)\n",
    "                        .otherwise(pl.col(y_col))\n",
    "                    ).to_numpy()\n",
    "\n",
    "                    truths = pred_df[f'{y_col}_label'].to_numpy()\n",
    "                    score = r2_score(truths, preds)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_th = th\n",
    "\n",
    "            if best_th is not None:\n",
    "                self.th_dict[y_col] = (best_th, best_score)\n",
    "\n",
    "\n",
    "    def create_oof_df(self, oof_df: pl.DataFrame):\n",
    "        oof_df = oof_df.select(self.sub_cols)\n",
    "        oof_df.write_parquet(self.config.oof_path / 'oof_pp.parquet')\n",
    "        return oof_df\n",
    "\n",
    "    def create_sub_df(self, sub_df: pl.DataFrame):\n",
    "        sub_df = sub_df.with_columns(sample_id = pl.concat_str([pl.lit('test_'), pl.col('sample_id')]))\n",
    "        sub_df = sub_df.select(self.sub_cols)\n",
    "        sub_df.write_csv(self.config.output_path / 'submission_pp.csv')\n",
    "        return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
