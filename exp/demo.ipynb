{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import seed_everything, get_logger, get_config, TimeUtil\n",
    "from src.utils.competition_utils import clipping_input\n",
    "from src.data import DataProvider, FeatureEngineering, Preprocessor, HFPreprocessor, PostProcessor\n",
    "from src.train import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = '146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:10:17\u001b[0m | \u001b[1mINFO ] exp: 146 | run_mode=hf, multi_task=False, loss_type=mae\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path('../config'))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f'exp: {exp} | run_mode={config.run_mode}, multi_task={config.multi_task}, loss_type={config.loss_type}')\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験のための変更\n",
    "config.run_mode = 'full'\n",
    "config.epochs = 40\n",
    "config.first_cycle_epochs = 40\n",
    "config.add_epochs = 10\n",
    "config.add_first_cycle_epochs = 10\n",
    "\n",
    "# ルートでディレクトリを作成されることに注意\n",
    "config.input_path = Path('../data/input')\n",
    "config.add_path = Path('../data/input/additional')\n",
    "config.output_path = Path(f'../data/output/{config.exp}')\n",
    "config.oof_path = Path(f'../data/oof/{config.exp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] start [0.5GB(2.2%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] done [73.8GB(18.9%)(+73.205GB)] 68.8733 s\n",
      "[Feature Engineering...] start [73.8GB(18.9%)]\n",
      "[Feature Engineering...] done [83.6GB(14.8%)(+9.826GB)] 17.7978 s\n",
      "[Scaling and Clipping Features...] start [83.6GB(14.8%)]\n",
      "[Scaling and Clipping Features...] done [75.0GB(10.0%)(-8.532GB)] 33.1949 s\n",
      "[Converting to arrays for NN...] start [75.0GB(10.0%)]\n",
      "[Converting to arrays for NN...] done [100.7GB(20.1%)(+25.655GB)] 116.7301 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Data Loading...'):\n",
    "    dpr = DataProvider(config)\n",
    "    train_df, test_df = dpr.load_data()\n",
    "\n",
    "\n",
    "with TimeUtil.timer('Feature Engineering...'):\n",
    "    fer = FeatureEngineering(config)\n",
    "    train_df = fer.feature_engineering(train_df)\n",
    "    test_df = fer.feature_engineering(test_df)\n",
    "\n",
    "\n",
    "with TimeUtil.timer('Scaling and Clipping Features...'):\n",
    "    ppr = Preprocessor(config)\n",
    "    train_df, test_df = ppr.scaling(train_df, test_df)\n",
    "    input_cols, target_cols = ppr.input_cols, ppr.target_cols\n",
    "    if config.task_type == 'grid_pred':\n",
    "        train_df = train_df.drop(target_cols)\n",
    "\n",
    "    valid_df = train_df.filter(pl.col('fold') == 0)\n",
    "    train_df = train_df.filter(pl.col('fold') != 0)\n",
    "    valid_df, input_clip_dict = clipping_input(train_df, valid_df, input_cols)\n",
    "    test_df, _ = clipping_input(None, test_df, input_cols, input_clip_dict)\n",
    "    pickle.dump(input_clip_dict, open(config.output_path / 'input_clip_dict.pkl', 'wb'))\n",
    "\n",
    "\n",
    "with TimeUtil.timer('Converting to arrays for NN...'):\n",
    "    array_data = ppr.convert_numpy_array(train_df, valid_df, test_df)\n",
    "    del train_df, valid_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if config.run_mode == 'hf':\n",
    "    with TimeUtil.timer('HF Data Preprocessing...'):\n",
    "        del array_data['train_ids'], array_data['X_train'], array_data['y_train']\n",
    "        gc.collect()\n",
    "\n",
    "        hf_ppr = HFPreprocessor(config)\n",
    "        hf_ppr.shrink_file_size()\n",
    "        hf_ppr.convert_numpy_array(unlink_parquet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Torch DataLoader...] start [100.7GB(20.0%)]\n",
      "[Creating Torch DataLoader...] done [100.7GB(20.0%)(+0.000GB)] 0.1369 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Creating Torch DataLoader...'):\n",
    "    if config.run_mode == 'hf':\n",
    "        train_loader = get_dataloader(\n",
    "            config,\n",
    "            hf_read_type='npy',\n",
    "            is_train=True\n",
    "        )\n",
    "    else:\n",
    "        train_loader = get_dataloader(\n",
    "            config,\n",
    "            array_data['train_ids'],\n",
    "            array_data['X_train'],\n",
    "            array_data['y_train'],\n",
    "            is_train=True\n",
    "        )\n",
    "    valid_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['valid_ids'],\n",
    "        array_data['X_valid'],\n",
    "        array_data['y_valid'],\n",
    "        is_train=False\n",
    "    )\n",
    "    test_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['test_ids'],\n",
    "        array_data['X_test'],\n",
    "        is_train=False\n",
    "    )\n",
    "    del array_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_loader.dataset\n",
    "# valid_dataset = valid_loader.dataset\n",
    "\n",
    "# train_dataset.X = train_dataset.X[:100000]\n",
    "# train_dataset.y = train_dataset.y[:100000]\n",
    "# train_dataset.ids = train_dataset.ids[:100000]\n",
    "# valid_dataset.X = valid_dataset.X[:100000]\n",
    "# valid_dataset.y = valid_dataset.y[:100000]\n",
    "# valid_dataset.ids = valid_dataset.ids[:100000]\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, batch_size=config.train_batch, shuffle=True, pin_memory=True, drop_last=True\n",
    "# )\n",
    "# valid_loader = DataLoader(\n",
    "#     valid_dataset, batch_size=config.eval_batch, shuffle=False, pin_memory=True, drop_last=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Literal, Tuple\n",
    "\n",
    "import loguru\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.train import ComponentFactory, ModelEmaV3, AverageMeter\n",
    "from src.utils import clean_message\n",
    "from src.utils.competition_utils import evaluate_metric, get_io_columns, get_sub_factor\n",
    "from src.utils.constant import (\n",
    "    PP_TARGET_COLS,\n",
    "    SCALER_TARGET_COLS,\n",
    "    TARGET_MIN_MAX,\n",
    "    VERTICAL_TARGET_COLS,\n",
    ")\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, save_suffix: str = \"\"):\n",
    "        self.config = config\n",
    "        self.eval_step = config.eval_step[config.run_mode]\n",
    "        self.logger = logger\n",
    "        self.save_suffix = save_suffix\n",
    "        self.detail_pbar = True\n",
    "\n",
    "        self.model = ComponentFactory.get_model(config)\n",
    "        self.model = self.model.to(config.device)\n",
    "        n_device = torch.cuda.device_count()\n",
    "        if n_device > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        if config.ema:\n",
    "            self.model_ema = ModelEmaV3(\n",
    "                self.model,\n",
    "                decay=config.ema_decay,\n",
    "                update_after_step = config.eval_step[config.run_mode] * 20,\n",
    "                device=config.device\n",
    "            )\n",
    "\n",
    "        self.loss_fn = ComponentFactory.get_loss(config)\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "\n",
    "        _, self.target_cols = get_io_columns(config)\n",
    "        self.model_target_cols = self.get_model_target_cols()\n",
    "        self.factor_dict = get_sub_factor(config.input_path, old=False)\n",
    "        self.old_factor_dict = get_sub_factor(config.input_path, old=True)\n",
    "\n",
    "        self.y_numerators = np.load(\n",
    "            config.output_path / f\"y_numerators_{config.target_scale_method}.npy\"\n",
    "        )\n",
    "        self.y_denominators = np.load(\n",
    "            config.output_path / f\"y_denominators_{config.target_scale_method}.npy\"\n",
    "        )\n",
    "        self.target_min_max = [TARGET_MIN_MAX[col] for col in self.target_cols]\n",
    "\n",
    "        self.pp_run = True\n",
    "        self.valid_ids = None\n",
    "        self.test_ids = None\n",
    "        self.valid_pp_df = None\n",
    "        self.test_pp_df = None\n",
    "        self.pp_y_cols = PP_TARGET_COLS\n",
    "        self.pp_x_cols = [col.replace(\"ptend\", \"state\") for col in self.pp_y_cols]\n",
    "\n",
    "        self.best_score_dict = defaultdict(lambda: (-1, -np.inf))\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        colwise_mode: bool = True,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str = \"\",\n",
    "        retrain_best_score: float = -np.inf,\n",
    "        eval_only: bool = False,\n",
    "    ):\n",
    "        if eval_only:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            eval_method = \"colwise\" if colwise_mode else \"single\"\n",
    "            score, cw_score, preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=eval_method\n",
    "            )\n",
    "            self.save_oof_df(self.valid_ids, preds)\n",
    "            return score, cw_score, -1\n",
    "\n",
    "        self.optimizer = ComponentFactory.get_optimizer(self.config, self.model)\n",
    "        steps_per_epoch = len(train_loader) if self.config.run_mode != 'hf' else self.config.eval_step[self.config.run_mode]\n",
    "        self.scheduler = ComponentFactory.get_scheduler(\n",
    "            self.config, self.optimizer, steps_per_epoch=steps_per_epoch\n",
    "        )\n",
    "        global_step = 0\n",
    "        eval_count = 0\n",
    "        best_score = -np.inf\n",
    "\n",
    "        if retrain:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\")\n",
    "            )\n",
    "            weight_numbers = [\n",
    "                int(file.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "                for file in list(self.config.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "            ]\n",
    "            eval_count = sorted(weight_numbers)[-1] + 1\n",
    "            best_score = retrain_best_score\n",
    "\n",
    "        # 学習ループの開始\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            iterations = (\n",
    "                tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            )\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(self.model, data, calc_loss=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                global_step += 1\n",
    "                if self.config.ema:\n",
    "                    self.model_ema.update(self.model, global_step)\n",
    "\n",
    "                if global_step % self.eval_step == 0:\n",
    "                    score, _, preds, update_num = self.valid_evaluate(\n",
    "                        valid_loader,\n",
    "                        current_epoch=epoch,\n",
    "                        eval_count=eval_count,\n",
    "                        eval_method=\"single\",\n",
    "                    )\n",
    "                    if colwise_mode and update_num > 0:\n",
    "                        parameters = self.model_ema.module.state_dict() if self.config.ema else self.model.state_dict()\n",
    "                        torch.save(\n",
    "                            parameters,\n",
    "                            self.config.output_path\n",
    "                            / f\"model{self.save_suffix}_eval{eval_count}.pth\",\n",
    "                        )\n",
    "\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preds = preds\n",
    "                        best_epochs = epoch\n",
    "                        parameters = self.model_ema.module.state_dict() if self.config.ema else self.model.state_dict()\n",
    "                        torch.save(\n",
    "                            parameters,\n",
    "                            self.config.output_path / f\"model{self.save_suffix}_best.pth\",\n",
    "                        )\n",
    "\n",
    "                    eval_count += 1\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.5f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.5e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "            if self.config.run_mode == 'hf':\n",
    "                train_loader = self.update_train_loader(train_loader)\n",
    "\n",
    "        if colwise_mode:\n",
    "            self.remove_unuse_weights()\n",
    "            best_score, best_cw_score, best_preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=\"colwise\"\n",
    "            )\n",
    "\n",
    "        self.save_oof_df(self.valid_ids, best_preds)\n",
    "        return best_score, best_cw_score, best_epochs\n",
    "\n",
    "    def valid_evaluate(\n",
    "        self,\n",
    "        valid_loader: DataLoader,\n",
    "        current_epoch: int,\n",
    "        eval_count: int,\n",
    "        eval_method: Literal[\"single\", \"colwise\"] = \"single\",\n",
    "    ):\n",
    "        if self.valid_ids is None:\n",
    "            self.valid_ids = valid_loader.dataset.ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            load_best_weight = True if eval_count == -1 else False\n",
    "            preds = self.inference_loop(\n",
    "                valid_loader, mode=\"valid\", load_best_weight=load_best_weight\n",
    "            )\n",
    "        elif eval_method == \"colwise\":\n",
    "            preds = self.inference_loop_colwise(valid_loader, \"valid\", self.best_score_dict)\n",
    "\n",
    "        labels = valid_loader.dataset.y\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            labels = self.convert_target_3dim_to_2dim(labels)\n",
    "        preds = self.restore_pred(preds)\n",
    "        labels = self.restore_pred(labels)\n",
    "\n",
    "        if self.pp_run and self.valid_pp_df is None:\n",
    "            self.load_postprocess_input(\"valid\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"valid\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        eval_idx = [\n",
    "            i for i, col in enumerate(self.target_cols) if self.factor_dict[col] != 0\n",
    "        ]  # factor_dictの値が0のものは自動でR2=1になるようにする\n",
    "        score, indiv_scores = evaluate_metric(preds, labels, eval_idx=eval_idx)\n",
    "        cw_score, update_num = self.update_best_score(indiv_scores, eval_count)\n",
    "\n",
    "        message = f\"\"\"\n",
    "            [Valid] :\n",
    "                Epoch={current_epoch},\n",
    "                Loss={self.valid_loss.avg:.5f},\n",
    "                Score={score:.5f},\n",
    "                Best Col-Wise Score={cw_score:.5f}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, cw_score, preds, update_num\n",
    "\n",
    "    def test_predict(\n",
    "        self, test_loader: DataLoader, eval_method: Literal[\"single\", \"colwise\"] = \"single\"\n",
    "    ):\n",
    "        if self.test_ids is None:\n",
    "            self.test_ids = test_loader.dataset.ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            preds = self.inference_loop(test_loader, mode=\"test\", load_best_weight=True)\n",
    "        elif eval_method == \"colwise\":\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            preds = self.inference_loop_colwise(test_loader, \"test\", self.best_score_dict)\n",
    "\n",
    "        preds = self.restore_pred(preds)\n",
    "        if self.pp_run and self.test_pp_df is None:\n",
    "            self.load_postprocess_input(\"test\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"test\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        pred_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        pred_df = pred_df.with_columns(sample_id=pl.Series(self.test_ids))\n",
    "        return pred_df\n",
    "\n",
    "    def forward_step(self, model: nn.Module, data: torch.Tensor, calc_loss: bool = True):\n",
    "        if calc_loss:\n",
    "            x, y = data\n",
    "            x, y = x.to(self.config.device), y.to(self.config.device)\n",
    "            out = model(x)\n",
    "            loss = self.loss_fn(out, y)\n",
    "        else:\n",
    "            x = data[0]\n",
    "            x = x.to(self.config.device)\n",
    "            out = model(x)\n",
    "            loss = None\n",
    "\n",
    "        if self.config.multi_task:\n",
    "            out = out[:, :, :self.config.out_dim]\n",
    "\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            out = self.convert_target_3dim_to_2dim(out)\n",
    "        return out, loss\n",
    "\n",
    "    def inference_loop(\n",
    "        self,\n",
    "        eval_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        load_best_weight: bool = False,\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\")\n",
    "            )\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = tqdm(eval_loader, total=len(eval_loader)) if self.detail_pbar else eval_loader\n",
    "            for data in iterations:\n",
    "                calc_loss = True if mode == \"valid\" else False\n",
    "                if load_best_weight or not self.config.ema:\n",
    "                    out, loss = self.forward_step(self.model, data, calc_loss=calc_loss)\n",
    "                else:\n",
    "                    out, loss = self.forward_step(self.model_ema, data, calc_loss=calc_loss)\n",
    "                if mode == \"valid\":\n",
    "                    self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                preds.append(out.detach().cpu().numpy())\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        return preds\n",
    "\n",
    "    def inference_loop_colwise(\n",
    "        self,\n",
    "        test_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        best_score_dict: dict[str, tuple[int, float]],\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        selected_counts = list(set([eval_count for eval_count, _ in best_score_dict.values()]))\n",
    "        all_preds = np.zeros((len(test_loader.dataset), len(self.target_cols)))\n",
    "        for eval_count in tqdm(selected_counts):\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    self.config.output_path / f\"model{self.save_suffix}_eval{eval_count}.pth\"\n",
    "                )\n",
    "            )\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                iterations = (\n",
    "                    tqdm(test_loader, total=len(test_loader)) if self.detail_pbar else test_loader\n",
    "                )\n",
    "                for data in iterations:\n",
    "                    calc_loss = True if mode == \"valid\" else False\n",
    "                    out, loss = self.forward_step(self.model, data, calc_loss=calc_loss)\n",
    "                    if mode == \"valid\":\n",
    "                        self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                    preds.append(out.detach().cpu().numpy())\n",
    "            preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "            target_cols = [\n",
    "                col for col, (count, _) in best_score_dict.items() if count == eval_count\n",
    "            ]\n",
    "            for col in target_cols:\n",
    "                idx = self.target_cols.index(col)\n",
    "                all_preds[:, idx] = preds[:, idx]\n",
    "        return all_preds\n",
    "\n",
    "    def update_train_loader(self, train_loader: DataLoader):\n",
    "        train_dataset = train_loader.dataset\n",
    "        train_dataset.update()\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.train_batch,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def update_best_score(self, indiv_scores: list[float], eval_count: int):\n",
    "        update_num = 0\n",
    "        for col, score in zip(self.target_cols, indiv_scores):\n",
    "            if score > self.best_score_dict[col][1] and eval_count != -1:\n",
    "                self.best_score_dict[col] = (eval_count, score)\n",
    "                update_num += 1\n",
    "\n",
    "        best_cw_score = (\n",
    "            np.sum([score for _, score in self.best_score_dict.values()])\n",
    "            + (368 - len(self.target_cols))\n",
    "        ) / 368\n",
    "        if update_num > 0 and eval_count != -1:\n",
    "            pickle.dump(\n",
    "                dict(self.best_score_dict),\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"wb\"),\n",
    "            )\n",
    "        return best_cw_score, update_num\n",
    "\n",
    "    def remove_unuse_weights(self):\n",
    "        selected_counts = set([v[0] for v in self.best_score_dict.values()])\n",
    "        weight_paths = list(self.config.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "        for path in weight_paths:\n",
    "            eval_count = int(path.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "            if eval_count not in selected_counts:\n",
    "                path.unlink()\n",
    "\n",
    "    def convert_target_3dim_to_2dim(\n",
    "        self, y: np.ndarray | torch.Tensor\n",
    "    ) -> np.ndarray | torch.Tensor:\n",
    "        y_v = y[:, :, : len(VERTICAL_TARGET_COLS)]\n",
    "        y_s = y[:, :, len(VERTICAL_TARGET_COLS) :]\n",
    "        if isinstance(y, np.ndarray):\n",
    "            y_v = np.transpose(y_v, (0, 2, 1)).reshape(y.shape[0], -1)\n",
    "            y_s = y_s.mean(axis=1)\n",
    "            y = np.concatenate([y_v, y_s], axis=-1)\n",
    "        elif isinstance(y, torch.Tensor):\n",
    "            y_v = y_v.permute(0, 2, 1).reshape(y.size(0), -1)\n",
    "            y_s = y_s.mean(dim=1)\n",
    "            y = torch.cat([y_v, y_s], dim=-1)\n",
    "\n",
    "        align_order = [self.model_target_cols.index(col) for col in self.target_cols]\n",
    "        assert len(y.shape) == 2\n",
    "        y = y[:, align_order]\n",
    "        return y\n",
    "\n",
    "    def get_model_target_cols(self):\n",
    "        model_target_cols = []\n",
    "        for col in VERTICAL_TARGET_COLS:\n",
    "            model_target_cols.extend([f\"{col}_{i}\" for i in range(60)])\n",
    "        for col in SCALER_TARGET_COLS:\n",
    "            model_target_cols.append(col)\n",
    "        return model_target_cols\n",
    "\n",
    "    def restore_pred(self, preds: np.ndarray):\n",
    "        return preds * self.y_denominators + self.y_numerators\n",
    "\n",
    "    def clipping_pred(self, preds: np.ndarray):\n",
    "        for i in range(preds.shape[1]):\n",
    "            preds[:, i] = np.clip(preds[:, i], self.target_min_max[i][0], self.target_min_max[i][1])\n",
    "        return preds\n",
    "\n",
    "    def save_oof_df(self, sample_ids: np.ndarray, preds: np.ndarray):\n",
    "        oof_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        oof_df = oof_df.with_columns(sample_id=pl.Series(sample_ids))\n",
    "        oof_df.write_parquet(self.config.oof_path / f\"oof{self.save_suffix}.parquet\")\n",
    "\n",
    "    def postprocess(self, preds: np.ndarray, run_type: Literal[\"valid\", \"test\"]):\n",
    "        pp_x = self.valid_pp_df if run_type == \"valid\" else self.test_pp_df\n",
    "        for x_col, y_col in zip(self.pp_x_cols, self.pp_y_cols):\n",
    "            if y_col in self.target_cols:\n",
    "                idx = self.target_cols.index(y_col)\n",
    "                old_factor = self.old_factor_dict[y_col] if self.config.mul_old_factor else 1\n",
    "                preds[:, idx] = (-1 * pp_x[x_col].to_numpy() / 1200) * old_factor\n",
    "        return preds\n",
    "\n",
    "    def load_postprocess_input(self, data_type: Literal[\"valid\", \"test\"]):\n",
    "        if data_type == \"valid\":\n",
    "            valid_path = (\n",
    "                self.config.input_path / \"18_shrinked.parquet\"\n",
    "                if self.config.shared_valid\n",
    "                else self.config.input_path / \"train_shrinked.parquet\"\n",
    "            )\n",
    "            self.valid_pp_df = (\n",
    "                pl.scan_parquet(valid_path)\n",
    "                .select([\"sample_id\"] + self.pp_x_cols)\n",
    "                .filter(pl.col(\"sample_id\").is_in(self.valid_ids))\n",
    "                .collect()\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.valid_ids})\n",
    "            self.valid_pp_df = id_df.join(self.valid_pp_df, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "        elif data_type == \"test\":\n",
    "            self.test_pp_df = pl.read_parquet(\n",
    "                self.config.input_path / \"test_shrinked.parquet\",\n",
    "                columns=[\"sample_id\"] + self.pp_x_cols,\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.test_ids})\n",
    "            self.test_pp_df = id_df.join(self.test_pp_df, on=\"sample_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd671c2924f480483fa0e4bb1160c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a1967a2ad64ce28f5e507632a70fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca54b1d90ef466380997d4e38e079be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:23:19\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.35267, Score=0.24368, Best Col-Wise Score=0.24368\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22300f519ee4a8aad59fc79a4327e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:23:38\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.32372, Score=0.28452, Best Col-Wise Score=0.28500\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3300b84868c47bc814a9f0b4e434701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:23:57\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.31091, Score=0.31131, Best Col-Wise Score=0.31193\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:23:58\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=0, Loss=0.31505, LR=2.45075e-04\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c070f732d3a945b9ba3cb43b013338db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1cfb5e41da4df8a3689a84cb8cde9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:24:16\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.30654, Score=0.32731, Best Col-Wise Score=0.32843\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4effc18877846f8b869eeb66b516f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:24:35\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.30455, Score=0.33983, Best Col-Wise Score=0.34113\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b06c6697834badb7139264d7dddb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:24:54\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.29690, Score=0.34767, Best Col-Wise Score=0.35006\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:24:58\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=1, Loss=0.26352, LR=4.85150e-04\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790f7c3276584b22aea404d9a1e74bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103580/2445575585.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4598fe4c7164d398821079874c66264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da26a22f31c4214affc5a48ec1ca10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faf892f0afd4118aede8b1854c8ea4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21073ce8107e4a7c9e923cbdbf17bda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07de3aa4ab04a84953bccb899514867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ae793354904a93939996a4cc185895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:26:04\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=-1, Loss=0.31588, Score=0.35006, Best Col-Wise Score=0.35006\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:26:05\u001b[0m | \u001b[1mINFO ] First Training Results: best_score=0.35005511632018715, best_cw_score=0.35005511632018715, best_epochs=1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config, logger)\n",
    "best_score, best_cw_score, best_epochs = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    colwise_mode=True,\n",
    ")\n",
    "logger.info(f'First Training Results: best_score={best_score}, best_cw_score={best_cw_score}, best_epochs={best_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.loss_type = config.add_loss_type\n",
    "config.epochs = config.add_epochs\n",
    "config.lr = config.add_lr\n",
    "config.first_cycle_epochs = config.add_first_cycle_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/output/146/model_eval5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103580/247072243.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ed026b8f6f4573b3f3071b46f049ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3477ee3967704c5b880dbbb8be8b25cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d419b68516804e18b9fd42aae6305602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:30:44\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.74184, Score=0.35944, Best Col-Wise Score=0.36092\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0fc3097ebe42778071370ab4652c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:31:02\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.73516, Score=0.36470, Best Col-Wise Score=0.36677\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e75719d42b047039f0e28c4d136804c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:31:21\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.72828, Score=0.36951, Best Col-Wise Score=0.37280\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:31:23\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=0, Loss=0.54147, LR=7.42500e-06\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d799ccf7e4c4628a161e686daacf060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ecf7963d884058a866aebb27d6169e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:31:41\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.72049, Score=0.37537, Best Col-Wise Score=0.37962\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f787b981ea5842158d0ea13985a28f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:32:00\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.71176, Score=0.38198, Best Col-Wise Score=0.38756\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7993671cf0e84a81944efe11e905c5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:32:19\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.70235, Score=0.38969, Best Col-Wise Score=0.39636\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:32:22\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=1, Loss=0.51795, LR=9.85000e-06\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12960ee497f84b9da350e4781226f0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103580/247072243.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cc0f43b0dd4d9d8a0fec8a1444bbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ef27766b8544e1b176c94f7fe48a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50e379c7c964c3bace6623eb2e749cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdc6db13b6d4cbc9e68bb0f3b2b3f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01472b54144457a21fd5b512d3c7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73442381bdf94d13b9697b200b07e414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3cf38e174c4e8da394b86fc205a63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f71ea2709c438bb62fc1026c12144d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febe6c0080704eb8a2f8cbb9c2b3a116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2280fbd7804072b21466e6aa56dca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:34:13\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=-1, Loss=0.75481, Score=0.39636, Best Col-Wise Score=0.39636\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:34:14\u001b[0m | \u001b[1mINFO ] Additional Training Results: best_score=0.3963600767076636, best_cw_score=0.3963600767076634, best_epochs=1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trained_weights = sorted(\n",
    "    config.output_path.glob(f\"model_eval*.pth\"),\n",
    "    key=lambda x: int(x.stem.split('_')[-1].replace('eval', ''))\n",
    ")\n",
    "\n",
    "trainer = Trainer(config, logger)\n",
    "best_score, best_cw_score, best_epochs = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    colwise_mode=True,\n",
    "    retrain=True,\n",
    "    retrain_weight_name=trained_weights[-1].stem,\n",
    "    retrain_best_score=best_score,\n",
    ")\n",
    "logger.info(f'Additional Training Results: best_score={best_score}, best_cw_score={best_cw_score}, best_epochs={best_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5258ecda2664eb9a408a42463357c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103580/247072243.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7328bdb66f469ba995d2e66cebacc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3593562a9cfa481f92628f2e8f0841b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de449398bb14e368602c6583e229cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c63c81e6c3475498f3f955ecb8b65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd2c789e3b34a0b88b0246cda71d6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e615ecac3aba41fe8c2e41e3d34ebf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71907af11e4f4c338474c4bc801fa401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a9c6a0f5bd4afbacfe926e77677fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be7e555b8e64677bc973cd93db94dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc13ed4d04c47eabf1282351d749280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_df = trainer.test_predict(test_loader, eval_method=\"colwise\")\n",
    "pred_df.write_csv(config.output_path / 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 294), (625000, 294))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_df = pl.read_parquet(config.oof_path / 'oof.parquet')\n",
    "pred_df = pl.read_csv(config.output_path / 'submission.csv', schema_overrides={'sample_id': pl.Int32})\n",
    "oof_df.shape, pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 05:07:28\u001b[0m | \u001b[1mINFO ] Complete! -->> OOF: (100000, 369), Submission: (625000, 369)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "por = PostProcessor(config, logger)\n",
    "oof_df, sub_df = por.postprocess(oof_df, pred_df)\n",
    "logger.info(f'OOF: {oof_df.shape}, Submission: {sub_df.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
