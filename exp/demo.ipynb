{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import seed_everything, get_logger, get_config, TimeUtil\n",
    "from src.utils.competition_utils import clipping_input\n",
    "from src.data import DataProvider, FeatureEngineering, Preprocessor, HFPreprocessor\n",
    "from src.train import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = '146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-09 15:21:40\u001b[0m | \u001b[1mINFO ] exp: 146 | run_mode=hf, multi_task=False, loss_type=mae\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path('../config'))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f'exp: {exp} | run_mode={config.run_mode}, multi_task={config.multi_task}, loss_type={config.loss_type}')\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.run_mode = 'dev'\n",
    "config.multi_task = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] start [0.6GB(3.7%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] done [69.1GB(20.8%)(+68.580GB)] 60.4235 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Data Loading...'):\n",
    "    dpr = DataProvider(config)\n",
    "    train_df, test_df = dpr.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature Engineering...] start [69.1GB(20.3%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature Engineering...] done [72.1GB(16.8%)(+2.950GB)] 26.2413 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Feature Engineering...'):\n",
    "    fer = FeatureEngineering(config)\n",
    "    train_df = fer.feature_engineering(train_df)\n",
    "    test_df = fer.feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scaling and Clipping Features...] start [72.1GB(16.8%)]\n",
      "[Scaling and Clipping Features...] done [36.2GB(13.0%)(-35.861GB)] 49.8249 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Scaling and Clipping Features...'):\n",
    "    ppr = Preprocessor(config)\n",
    "    train_df, test_df = ppr.scaling(train_df, test_df)\n",
    "    input_cols, target_cols = ppr.input_cols, ppr.target_cols\n",
    "    if config.task_type == 'grid_pred':\n",
    "        train_df = train_df.drop(target_cols)\n",
    "\n",
    "    valid_df = train_df.filter(pl.col('fold') == 0)\n",
    "    train_df = train_df.filter(pl.col('fold') != 0)\n",
    "    valid_df, input_clip_dict = clipping_input(train_df, valid_df, input_cols)\n",
    "    test_df, _ = clipping_input(None, test_df, input_cols, input_clip_dict)\n",
    "    pickle.dump(input_clip_dict, open(config.output_path / 'input_clip_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Converting to arrays for NN...] start [36.2GB(12.9%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Converting to arrays for NN...] done [54.3GB(22.3%)(+18.043GB)] 182.7205 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Converting to arrays for NN...'):\n",
    "    array_data = ppr.convert_numpy_array(train_df, valid_df, test_df)\n",
    "    del train_df, valid_df, test_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare HF Data\n",
    "if config.run_mode == 'hf':\n",
    "    with TimeUtil.timer('HF Data Preprocessing...'):\n",
    "        hf_ppr = HFPreprocessor(config)\n",
    "        # hf_pcr.preprocess_data()\n",
    "        # hf_pcr.convert_numpy_array(near_target=False)\n",
    "        # del train_loader; gc.collect()\n",
    "        # train_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Torch DataLoader...] start [54.3GB(21.6%)]\n",
      "[Creating Torch DataLoader...] done [54.3GB(21.6%)(+0.000GB)] 0.1580 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Creating Torch DataLoader...'):\n",
    "    train_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['train_ids'],\n",
    "        array_data['X_train'],\n",
    "        array_data['y_train'],\n",
    "        is_train=True\n",
    "    )\n",
    "    valid_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['valid_ids'],\n",
    "        array_data['X_valid'],\n",
    "        array_data['y_valid'],\n",
    "        is_train=False\n",
    "    )\n",
    "    test_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['test_ids'],\n",
    "        array_data['X_test'],\n",
    "        is_train=False\n",
    "    )\n",
    "    del array_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Literal, Tuple\n",
    "\n",
    "import loguru\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.train import ComponentFactory\n",
    "from src.train.train_utils import AverageMeter\n",
    "from src.utils import clean_message\n",
    "from src.utils.competition_utils import evaluate_metric, get_io_columns, get_sub_factor\n",
    "from src.utils.constant import (\n",
    "    PP_TARGET_COLS,\n",
    "    SCALER_TARGET_COLS,\n",
    "    TARGET_MIN_MAX,\n",
    "    VERTICAL_TARGET_COLS,\n",
    ")\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: DictConfig, logger: loguru._Logger, save_suffix: str = \"\"):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.save_suffix = save_suffix\n",
    "        self.detail_pbar = True\n",
    "\n",
    "        self.model = ComponentFactory.get_model(config)\n",
    "        self.model = self.model.to(config.device)\n",
    "        n_device = torch.cuda.device_count()\n",
    "        if n_device > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        self.loss_fn = ComponentFactory.get_loss(config)\n",
    "        self.train_loss = AverageMeter()\n",
    "        self.valid_loss = AverageMeter()\n",
    "\n",
    "        _, self.target_cols = get_io_columns(config)\n",
    "        self.model_target_cols = self.get_model_target_cols()\n",
    "        self.old_factor_dict = get_sub_factor(config.input_path, old=True)\n",
    "\n",
    "        self.y_numerators = np.load(config.output_path / f\"y_numerators_{config.target_scale_method}.npy\")\n",
    "        self.y_denominators = np.load(config.output_path / f\"y_denominators_{config.target_scale_method}.npy\")\n",
    "        self.target_min_max = [TARGET_MIN_MAX[col] for col in self.target_cols]\n",
    "\n",
    "        self.pp_run = True\n",
    "        self.pp_y_cols = PP_TARGET_COLS\n",
    "        self.pp_x_cols = [col.replace(\"ptend\", \"state\") for col in self.pp_y_cols]\n",
    "\n",
    "        self.best_score_dict = defaultdict(lambda: (-1, -np.inf))\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        colwise_mode: bool = True,\n",
    "        retrain: bool = False,\n",
    "        retrain_weight_name: str = \"\",\n",
    "        retrain_best_score: float = -np.inf,\n",
    "        eval_only: bool = False,\n",
    "    ):\n",
    "        if eval_only:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            eval_method = \"colwise\" if colwise_mode else \"single\"\n",
    "            score, cw_score, preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=eval_method\n",
    "            )\n",
    "            self.save_oof_df(preds, self.valid_ids)\n",
    "            return score, cw_score, -1\n",
    "\n",
    "        self.optimizer = ComponentFactory.get_optimizer(config, self.model)\n",
    "        self.scheduler = ComponentFactory.get_scheduler(config, self.optimizer, steps_per_epoch=len(train_loader))\n",
    "\n",
    "        global_step = 0\n",
    "        eval_count = 0\n",
    "        best_score = -np.inf\n",
    "\n",
    "        if retrain:\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"{retrain_weight_name}.pth\")\n",
    "            )\n",
    "            weight_numbers = [\n",
    "                int(file.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "                for file in list(self.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "            ]\n",
    "            eval_count = sorted(weight_numbers)[-1] + 1\n",
    "            best_score = retrain_best_score\n",
    "\n",
    "        for epoch in tqdm(range(self.config.epochs)):\n",
    "            self.model.train()\n",
    "            self.train_loss.reset()\n",
    "\n",
    "            iterations = (\n",
    "                tqdm(train_loader, total=len(train_loader)) if self.detail_pbar else train_loader\n",
    "            )\n",
    "            for data in iterations:\n",
    "                _, loss = self.forward_step(data, calc_loss=True)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.train_loss.update(loss.item(), n=data[0].size(0))\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % self.config.eval_step == 0:\n",
    "                    score, _, preds, update_num = self.valid_evaluate(\n",
    "                        valid_loader,\n",
    "                        current_epoch=epoch,\n",
    "                        eval_count=eval_count,\n",
    "                        eval_method=\"single\",\n",
    "                    )\n",
    "                    if colwise_mode and update_num > 0:\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            self.config.output_path\n",
    "                            / f\"model{self.save_suffix}_eval{eval_count}.pth\",\n",
    "                        )\n",
    "\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_preds = preds\n",
    "                        best_epochs = epoch\n",
    "                        torch.save(\n",
    "                            self.model.state_dict(),\n",
    "                            self.config.output_path / f\"model{self.save_suffix}_best.pth\",\n",
    "                        )\n",
    "\n",
    "                    eval_count += 1\n",
    "                    self.model.train()\n",
    "\n",
    "            message = f\"\"\"\n",
    "                [Train] :\n",
    "                    Epoch={epoch},\n",
    "                    Loss={self.train_loss.avg:.5f},\n",
    "                    LR={self.optimizer.param_groups[0][\"lr\"]:.5e}\n",
    "            \"\"\"\n",
    "            self.logger.info(clean_message(message))\n",
    "\n",
    "        if colwise_mode:\n",
    "            self.remove_unuse_weights()\n",
    "            best_score, best_cw_score, best_preds, _ = self.valid_evaluate(\n",
    "                valid_loader, current_epoch=-1, eval_count=-1, eval_method=\"colwise\"\n",
    "            )\n",
    "\n",
    "        self.save_oof_df(best_preds, self.valid_ids)\n",
    "        return best_score, best_cw_score, best_epochs\n",
    "\n",
    "    def valid_evaluate(\n",
    "        self,\n",
    "        valid_loader: DataLoader,\n",
    "        current_epoch: int,\n",
    "        eval_count: int,\n",
    "        eval_method: Literal[\"single\", \"colwise\"] = \"single\",\n",
    "    ):\n",
    "        if self.valid_ids is None:\n",
    "            self.valid_ids = valid_loader.dataset.sample_ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            preds = self.inference_loop(valid_loader, mode=\"valid\", load_best_weight=False)\n",
    "        elif eval_method == \"colwise\":\n",
    "            preds = self.inference_loop_colwise(valid_loader, \"valid\", self.best_score_dict)\n",
    "\n",
    "        labels = valid_loader.dataset.y\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            labels = self.convert_target_3dim_to_2dim(labels)\n",
    "        preds = self.restore_pred(preds)\n",
    "        labels = self.restore_pred(labels)\n",
    "\n",
    "        if self.pp_run and self.valid_pp_x is None:\n",
    "            self.load_postprocess_input(\"valid\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"valid\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        eval_idx = [\n",
    "            i for i, col in enumerate(self.target_cols) if self.factor_dict[col] != 0\n",
    "        ]  # factor_dictの値が0のものは自動でR2=1になるようにする\n",
    "        score, indiv_scores = evaluate_metric(preds, labels, eval_idx=eval_idx)\n",
    "        cw_score, update_num = self.update_best_score(indiv_scores, eval_count)\n",
    "\n",
    "        message = f\"\"\"\n",
    "            [Valid] :\n",
    "                Epoch={current_epoch},\n",
    "                Loss={self.valid_loss.avg:.5f},\n",
    "                Score={score:.5f},\n",
    "                Best Col-Wise Score={cw_score:.5f}\n",
    "        \"\"\"\n",
    "        self.logger.info(clean_message(message))\n",
    "        return score, cw_score, preds, update_num\n",
    "\n",
    "    def test_predict(\n",
    "        self, test_loader: DataLoader, eval_method: Literal[\"single\", \"colwise\"] = \"single\"\n",
    "    ):\n",
    "        if self.test_ids is None:\n",
    "            self.test_ids = test_loader.dataset.sample_ids\n",
    "\n",
    "        if eval_method == \"single\":\n",
    "            preds = self.inference_loop(test_loader, mode=\"test\", load_best_weight=True)\n",
    "        elif eval_method == \"colwise\":\n",
    "            self.best_score_dict = pickle.load(\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"rb\")\n",
    "            )\n",
    "            preds = self.inference_loop_colwise(test_loader, \"test\", self.best_score_dict)\n",
    "\n",
    "        preds = self.restore_pred(preds)\n",
    "        if self.pp_run and self.test_pp_x is None:\n",
    "            self.load_postprocess_input(\"test\")\n",
    "        if self.pp_run:\n",
    "            preds = self.postprocess(preds, run_type=\"test\")\n",
    "        if self.config.out_clip:\n",
    "            preds = self.clipping_pred(preds)\n",
    "\n",
    "        pred_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        pred_df = pred_df.with_columns(sample_id=pl.Series(self.test_ids))\n",
    "        return pred_df\n",
    "\n",
    "    def inference_loop(\n",
    "        self,\n",
    "        eval_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        load_best_weight: bool = False,\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        # テストデータを推論するときはbest_weightを読み込む\n",
    "        if load_best_weight:\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\")\n",
    "            )\n",
    "\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            iterations = (\n",
    "                tqdm(eval_loader, total=len(eval_loader)) if self.detail_pbar else eval_loader\n",
    "            )\n",
    "            for data in iterations:\n",
    "                if mode == \"valid\":\n",
    "                    out, loss = self.forward_step(data, calc_loss=True)\n",
    "                    self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                elif mode == \"test\":\n",
    "                    out, _ = self.forward_step(data, calc_loss=False)\n",
    "                preds.append(out.detach().cpu().numpy())\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        return preds\n",
    "\n",
    "    def inference_loop_colwise(\n",
    "        self,\n",
    "        test_loader: DataLoader,\n",
    "        mode: Literal[\"valid\", \"test\"],\n",
    "        best_score_dict: dict[str, tuple[int, float]],\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        if mode == \"valid\":\n",
    "            self.valid_loss.reset()\n",
    "\n",
    "        selected_counts = list(set([eval_count for eval_count, _ in best_score_dict.values()]))\n",
    "        all_preds = np.zeros((len(test_loader.dataset), len(self.target_cols)))\n",
    "        for eval_count in tqdm(selected_counts):\n",
    "            self.model.load_state_dict(\n",
    "                torch.load(\n",
    "                    self.config.output_path / f\"model{self.save_suffix}_eval{eval_count}.pth\"\n",
    "                )\n",
    "            )\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                iterations = (\n",
    "                    tqdm(test_loader, total=len(test_loader)) if self.detail_pbar else test_loader\n",
    "                )\n",
    "                for data in iterations:\n",
    "                    if mode == \"valid\":\n",
    "                        out, loss = self.forward_step(data, calc_loss=True)\n",
    "                        self.valid_loss.update(loss.item(), n=data[0].size(0))\n",
    "                    elif mode == \"test\":\n",
    "                        out, _ = self.forward_step(data, calc_loss=False)\n",
    "                    preds.append(out.detach().cpu().numpy())\n",
    "            preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "            target_cols = [\n",
    "                col for col, (count, _) in best_score_dict.items() if count == eval_count\n",
    "            ]\n",
    "            for col in target_cols:\n",
    "                idx = self.target_cols.index(col)\n",
    "                all_preds[:, idx] = preds[:, idx]\n",
    "        return all_preds\n",
    "\n",
    "    def update_best_score(self, indiv_scores: list[float], eval_count: int):\n",
    "        update_num = 0\n",
    "        for col, score in zip(self.target_cols, indiv_scores):\n",
    "            if score > self.best_score_dict[col][1] and eval_count != -1:\n",
    "                self.best_score_dict[col] = (eval_count, score)\n",
    "                update_num += 1\n",
    "\n",
    "        best_cw_score = (\n",
    "            np.sum([score for _, score in self.best_score_dict.values()])\n",
    "            + (368 - len(self.target_cols))\n",
    "        ) / 368\n",
    "        if update_num > 0 and eval_count != -1:\n",
    "            pickle.dump(\n",
    "                self.best_score_dict,\n",
    "                open(self.config.output_path / f\"best_score_dict{self.save_suffix}.pkl\", \"wb\"),\n",
    "            )\n",
    "        return best_cw_score, update_num\n",
    "\n",
    "    def remove_unuse_weights(self):\n",
    "        selected_counts = set([v[0] for v in self.best_score_dict.values()])\n",
    "        weight_paths = list(self.config.output_path.glob(f\"model{self.save_suffix}_eval*.pth\"))\n",
    "        for path in weight_paths:\n",
    "            eval_count = int(path.stem.split(\"_\")[-1].replace(\"eval\", \"\"))\n",
    "            if eval_count not in selected_counts:\n",
    "                path.unlink()\n",
    "\n",
    "    def forward_step(self, data: torch.Tensor, calc_loss: bool = True):\n",
    "        if calc_loss:\n",
    "            x, y = data\n",
    "            x, y = x.to(self.config.device), y.to(self.config.device)\n",
    "            out = self.model(x)\n",
    "            loss = self.loss_fn(out, y)\n",
    "        else:\n",
    "            x = data\n",
    "            x = x.to(self.config.device)\n",
    "            out = self.model(x)\n",
    "            loss = None\n",
    "\n",
    "        if self.config.target_shape == \"3dim\":\n",
    "            out = self.convert_target_3dim_to_2dim(out)\n",
    "        return out, loss\n",
    "\n",
    "    def convert_target_3dim_to_2dim(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        y_v = y[:, :, : len(VERTICAL_TARGET_COLS)]\n",
    "        y_s = y[:, :, len(VERTICAL_TARGET_COLS) :]\n",
    "        y_v = y_v.permute(0, 2, 1).reshape(y.size(0), -1)\n",
    "        y_s = y_s.mean(dim=1)\n",
    "        y = torch.cat([y_v, y_s], dim=-1)\n",
    "        y = self.alignment_target_idx(y)\n",
    "        return y\n",
    "\n",
    "    def alignment_target_idx(self, y: np.ndarray | torch.Tensor) -> np.ndarray | torch.Tensor:\n",
    "        \"\"\"\n",
    "        target_colsとモデルの出力の順番を合わせる\n",
    "        \"\"\"\n",
    "        align_order = [self.model_target_cols.index(col) for col in self.target_cols]\n",
    "        assert len(y.shape) == 2\n",
    "        y = y[:, align_order]\n",
    "        return y\n",
    "\n",
    "    def get_model_target_cols(self):\n",
    "        model_target_cols = []\n",
    "        for col in VERTICAL_TARGET_COLS:\n",
    "            model_target_cols.extend([f\"{col}_{i}\" for i in range(60)])\n",
    "        for col in SCALER_TARGET_COLS:\n",
    "            model_target_cols.append(col)\n",
    "        return model_target_cols\n",
    "\n",
    "    def restore_pred(self, preds: np.ndarray):\n",
    "        return preds * self.config.y_denominators + self.config.y_numerators\n",
    "\n",
    "    def clipping_pred(self, preds: np.ndarray):\n",
    "        for i in range(preds.shape[1]):\n",
    "            preds[:, i] = np.clip(preds[:, i], self.target_min_max[i][0], self.target_min_max[i][1])\n",
    "        return preds\n",
    "\n",
    "    def save_oof_df(self, sample_ids: np.ndarray, preds: np.ndarray):\n",
    "        oof_df = pl.DataFrame(preds, schema=self.target_cols)\n",
    "        oof_df = oof_df.with_columns(sample_id=pl.Series(sample_ids))\n",
    "        oof_df.write_parquet(self.config.oof_path / f\"oof{self.save_suffix}.parquet\")\n",
    "\n",
    "    def postprocess(self, preds: np.ndarray, run_type: Literal[\"valid\", \"test\"]):\n",
    "        pp_x = self.valid_pp_x if run_type == \"valid\" else self.test_pp_x\n",
    "        for x_col, y_col in zip(self.pp_x_cols, self.pp_y_cols):\n",
    "            if y_col in self.target_cols:\n",
    "                idx = self.target_cols.index(y_col)\n",
    "                old_factor = self.old_factor_dict[y_col] if self.config.mul_old_factor else 1\n",
    "                preds[:, idx] = (-1 * pp_x[x_col].to_numpy() / 1200) * old_factor\n",
    "        return preds\n",
    "\n",
    "    def load_postprocess_input(self, data_type: Literal[\"valid\", \"test\"]):\n",
    "        if data_type == \"valid\":\n",
    "            valid_path = (\n",
    "                self.config.input_path / \"18_shrinked.parquet\"\n",
    "                if self.config.shared_valid\n",
    "                else self.config.input_path / \"train_shrinked.parquet\"\n",
    "            )\n",
    "            self.valid_pp_x = (\n",
    "                pl.scan_parquet(valid_path)\n",
    "                .select([\"sample_id\"] + self.pp_x_cols)\n",
    "                .filter(pl.col(\"sample_id\").is_in(self.valid_ids))\n",
    "                .collect()\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.valid_ids})\n",
    "            self.valid_pp_x = id_df.join(self.valid_pp_x, on=\"sample_id\", how=\"left\")\n",
    "\n",
    "        elif data_type == \"test\":\n",
    "            self.test_pp_x = pl.read_parquet(\n",
    "                self.config.input_path / \"test_shrinked.parquet\",\n",
    "                columns=[\"sample_id\"] + self.pp_x_cols,\n",
    "            )\n",
    "            id_df = pl.DataFrame({\"sample_id\": self.test_ids})\n",
    "            self.test_pp_x = id_df.join(self.test_pp_x, on=\"sample_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dee47603704e9dbb3c06b04c80fff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1588fbc7b847d39c98ee4521f6c659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 988.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 134.75 MiB is free. Process 1433027 has 7.51 GiB memory in use. Of the allocated memory 6.51 GiB is allocated by PyTorch, and 900.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_score, best_cw_score, best_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolwise_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 108\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, valid_loader, colwise_mode, retrain, retrain_weight_name, retrain_best_score, eval_only)\u001b[0m\n\u001b[1;32m    106\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_step(data, calc_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 108\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/kaggle-leap-atmospheric-physics-ai-climsim/.venv/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kaggle-leap-atmospheric-physics-ai-climsim/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kaggle-leap-atmospheric-physics-ai-climsim/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 988.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 134.75 MiB is free. Process 1433027 has 7.51 GiB memory in use. Of the allocated memory 6.51 GiB is allocated by PyTorch, and 900.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "best_score, best_cw_score, best_epochs = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    colwise_mode=True,\n",
    "    eval_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
