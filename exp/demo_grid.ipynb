{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path, PosixPath\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import seed_everything, get_logger, get_config, TimeUtil\n",
    "from src.utils.competition_utils import clipping_input\n",
    "from src.data import DataProvider, FeatureEngineering, Preprocessor, HFPreprocessor, PostProcessor\n",
    "from src.train import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドライン引数\n",
    "exp = '146'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 12:35:28\u001b[0m | \u001b[1mINFO ] exp: 146 | run_mode=hf, multi_task=False, loss_type=mae\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = get_config(exp, config_dir=Path('../config'))\n",
    "logger = get_logger(config.output_path)\n",
    "logger.info(f'exp: {exp} | run_mode={config.run_mode}, multi_task={config.multi_task}, loss_type={config.loss_type}')\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験のための変更\n",
    "config.run_mode = 'full'\n",
    "config.epochs = 40\n",
    "config.first_cycle_epochs = 40\n",
    "config.add_epochs = 10\n",
    "config.add_first_cycle_epochs = 10\n",
    "\n",
    "# ルートでディレクトリを作成されることに注意\n",
    "config.input_path = Path('../data/input')\n",
    "config.add_path = Path('../data/input/additional')\n",
    "config.output_path = Path(f'../data/output/{config.exp}')\n",
    "config.oof_path = Path(f'../data/oof/{config.exp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] start [0.5GB(2.0%)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Loading...] done [109.1GB(45.1%)(+108.539GB)] 45.7564 s\n",
      "[Feature Engineering...] start [109.1GB(45.1%)]\n",
      "[Feature Engineering...] done [133.5GB(33.6%)(+24.375GB)] 42.7129 s\n",
      "[Scaling and Clipping Features...] start [133.5GB(33.6%)]\n",
      "[Scaling and Clipping Features...] done [74.7GB(23.9%)(-58.727GB)] 90.1228 s\n",
      "[Converting to arrays for NN...] start [74.7GB(23.9%)]\n",
      "[Converting to arrays for NN...] done [118.6GB(46.7%)(+43.845GB)] 318.8203 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Data Loading...'):\n",
    "    dpr = DataProvider(config)\n",
    "    train_df, test_df = dpr.load_data()\n",
    "\n",
    "\n",
    "with TimeUtil.timer('Feature Engineering...'):\n",
    "    fer = FeatureEngineering(config)\n",
    "    train_df = fer.feature_engineering(train_df)\n",
    "    test_df = fer.feature_engineering(test_df)\n",
    "\n",
    "\n",
    "with TimeUtil.timer('Scaling and Clipping Features...'):\n",
    "    ppr = Preprocessor(config)\n",
    "    train_df, test_df = ppr.scaling(train_df, test_df)\n",
    "    input_cols, target_cols = ppr.input_cols, ppr.target_cols\n",
    "    if config.task_type == 'grid_pred':\n",
    "        train_df = train_df.drop(target_cols)\n",
    "\n",
    "    valid_df = train_df.filter(pl.col('fold') == 0)\n",
    "    train_df = train_df.filter(pl.col('fold') != 0)\n",
    "    valid_df, input_clip_dict = clipping_input(train_df, valid_df, input_cols)\n",
    "    test_df, _ = clipping_input(None, test_df, input_cols, input_clip_dict)\n",
    "    pickle.dump(input_clip_dict, open(config.output_path / 'input_clip_dict.pkl', 'wb'))\n",
    "\n",
    "\n",
    "with TimeUtil.timer('Converting to arrays for NN...'):\n",
    "    array_data = ppr.convert_numpy_array(train_df, valid_df, test_df)\n",
    "    del train_df, valid_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if config.run_mode == 'hf':\n",
    "    with TimeUtil.timer('HF Data Preprocessing...'):\n",
    "        del array_data['train_ids'], array_data['X_train'], array_data['y_train']\n",
    "        gc.collect()\n",
    "\n",
    "        hf_ppr = HFPreprocessor(config)\n",
    "        hf_ppr.shrink_file_size()\n",
    "        hf_ppr.convert_numpy_array(unlink_parquet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Creating Torch DataLoader...] start [118.6GB(46.7%)]\n",
      "[Creating Torch DataLoader...] done [118.6GB(46.5%)(+0.000GB)] 0.1261 s\n"
     ]
    }
   ],
   "source": [
    "with TimeUtil.timer('Creating Torch DataLoader...'):\n",
    "    if config.run_mode == 'hf':\n",
    "        train_loader = get_dataloader(\n",
    "            config,\n",
    "            hf_read_type='npy',\n",
    "            is_train=True\n",
    "        )\n",
    "    else:\n",
    "        train_loader = get_dataloader(\n",
    "            config,\n",
    "            array_data['train_ids'],\n",
    "            array_data['X_train'],\n",
    "            array_data['y_train'],\n",
    "            is_train=True\n",
    "        )\n",
    "    valid_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['valid_ids'],\n",
    "        array_data['X_valid'],\n",
    "        array_data['y_valid'],\n",
    "        is_train=False\n",
    "    )\n",
    "    test_loader = get_dataloader(\n",
    "        config,\n",
    "        array_data['test_ids'],\n",
    "        array_data['X_test'],\n",
    "        is_train=False\n",
    "    )\n",
    "    del array_data\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from src.model.models.base import BaseModel\n",
    "\n",
    "\n",
    "class LEAPSqueezeformer(BaseModel):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from squeezeformer.encoder import SqueezeformerEncoder\n",
    "\n",
    "\n",
    "class Squeezeformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeezeformer incorporates the Temporal U-Net structure, which reduces the cost of the\n",
    "    multi-head attention modules on long sequences, and a simpler block structure of feed-forward module,\n",
    "    followed up by multi-head attention or convolution modules,\n",
    "    instead of the Macaron structure proposed in Conformer.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classification classes\n",
    "        input_dim (int, optional): Dimension of input vector\n",
    "        encoder_dim (int, optional): Dimension of squeezeformer encoder\n",
    "        num_encoder_layers (int, optional): Number of squeezeformer blocks\n",
    "        reduce_layer_index (int, optional): The layer index to reduce sequence length\n",
    "        recover_layer_index (int, optional): The layer index to recover sequence length\n",
    "        num_attention_heads (int, optional): Number of attention heads\n",
    "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
    "        conv_expansion_factor (int, optional): Expansion factor of squeezeformer convolution module\n",
    "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
    "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
    "        conv_dropout_p (float, optional): Probability of squeezeformer convolution module dropout\n",
    "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
    "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "        - **input_lengths** (batch): list of sequence input lengths\n",
    "    Returns: outputs, output_lengths\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by squeezeformer.\n",
    "        - **output_lengths** (batch): list of sequence output lengths\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        input_dim: int = 80,\n",
    "        encoder_dim: int = 512,\n",
    "        num_encoder_layers: int = 16,\n",
    "        reduce_layer_index: int = 7,\n",
    "        recover_layer_index: int = 15,\n",
    "        num_attention_heads: int = 8,\n",
    "        feed_forward_expansion_factor: int = 4,\n",
    "        conv_expansion_factor: int = 2,\n",
    "        input_dropout_p: float = 0.1,\n",
    "        feed_forward_dropout_p: float = 0.1,\n",
    "        attention_dropout_p: float = 0.1,\n",
    "        conv_dropout_p: float = 0.1,\n",
    "        conv_kernel_size: int = 31,\n",
    "        half_step_residual: bool = False,\n",
    "    ) -> None:\n",
    "        super(Squeezeformer, self).__init__()\n",
    "        self.encoder = SqueezeformerEncoder(\n",
    "            input_dim=input_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            num_layers=num_encoder_layers,\n",
    "            reduce_layer_index=reduce_layer_index,\n",
    "            recover_layer_index=recover_layer_index,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "            conv_expansion_factor=conv_expansion_factor,\n",
    "            input_dropout_p=input_dropout_p,\n",
    "            feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "            attention_dropout_p=attention_dropout_p,\n",
    "            conv_dropout_p=conv_dropout_p,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            half_step_residual=half_step_residual,\n",
    "        )\n",
    "        self.fc = nn.Linear(encoder_dim, num_classes, bias=False)\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"Count parameters of encoder\"\"\"\n",
    "        return self.encoder.count_parameters()\n",
    "\n",
    "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Forward propagate a `inputs` and `targets` pair for training.\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "        Returns:\n",
    "            * predictions (torch.FloatTensor): Result of model predictions.\n",
    "        \"\"\"\n",
    "        encoder_outputs, encoder_output_lengths = self.encoder(inputs, input_lengths)\n",
    "        outputs = self.fc(encoder_outputs)\n",
    "        outputs = F.log_softmax(outputs, dim=-1)\n",
    "        return outputs, encoder_output_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from squeezeformer.activation import GLU, Swish\n",
    "from squeezeformer.modules import Transpose\n",
    "\n",
    "\n",
    "class DepthwiseConv2dSubsampling(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise Convolutional 2D subsampling (to 1/4 length)\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input image\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "    Inputs: inputs, input_lengths\n",
    "        - **inputs** (batch, time, dim): Tensor containing sequence of inputs\n",
    "        - **input_lengths** (batch): list of sequence input lengths\n",
    "    Returns: outputs, output_lengths\n",
    "        - **outputs** (batch, time, dim): Tensor produced by the convolution\n",
    "        - **output_lengths** (batch): list of sequence output lengths\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super(DepthwiseConv2dSubsampling, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            DepthwiseConv2d(out_channels, out_channels, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        outputs = self.sequential(inputs.unsqueeze(1))\n",
    "        batch_size, channels, subsampled_lengths, subsampled_dim = outputs.size()\n",
    "\n",
    "        outputs = outputs.permute(0, 2, 1, 3)\n",
    "        outputs = outputs.contiguous().view(batch_size, subsampled_lengths, channels * subsampled_dim)\n",
    "\n",
    "        output_lengths = input_lengths >> 2\n",
    "        output_lengths -= 1\n",
    "\n",
    "        return outputs, output_lengths\n",
    "\n",
    "\n",
    "class DepthwiseConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    When groups == in_channels and out_channels == K * in_channels, where K is a positive integer,\n",
    "    this operation is termed in literature as depthwise convolution.\n",
    "    ref : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (int or tuple): Size of the convolving kernel\n",
    "        stride (int, optional): Stride of the convolution. Default: 2\n",
    "        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, in_channels, time): Tensor containing input vector\n",
    "    Returns: outputs\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by depthwise 2-D convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple],\n",
    "        stride: int = 2,\n",
    "        padding: int = 0,\n",
    "    ) -> None:\n",
    "        super(DepthwiseConv2d, self).__init__()\n",
    "        assert out_channels % in_channels == 0, \"out_channels should be constant multiple of in_channels\"\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=in_channels,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "class DepthwiseConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    When groups == in_channels and out_channels == K * in_channels, where K is a positive integer,\n",
    "    this operation is termed in literature as depthwise convolution.\n",
    "    ref : https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        stride (int, optional): Stride of the convolution. Default: 1\n",
    "        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "        bias (bool, optional): If True, adds a learnable bias to the output. Default: False\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, in_channels, time): Tensor containing input vector\n",
    "    Returns: outputs\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by depthwise 1-D convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        bias: bool = False,\n",
    "    ) -> None:\n",
    "        super(DepthwiseConv1d, self).__init__()\n",
    "        assert out_channels % in_channels == 0, \"out_channels should be constant multiple of in_channels\"\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=in_channels,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "class PointwiseConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    When kernel size == 1 conv1d, this operation is termed in literature as pointwise convolution.\n",
    "    This operation often used to match dimensions.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        stride (int, optional): Stride of the convolution. Default: 1\n",
    "        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "        bias (bool, optional): If True, adds a learnable bias to the output. Default: True\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, in_channels, time): Tensor containing input vector\n",
    "    Returns: outputs\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by pointwise 1-D convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super(PointwiseConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution module starts with a pointwise convolution and a gated linear unit (GLU).\n",
    "    This is followed by a single 1-D depthwise convolution layer. Batchnorm is deployed just after the convolution\n",
    "    to aid training deep models.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input\n",
    "        kernel_size (int or tuple, optional): Size of the convolving kernel Default: 31\n",
    "        dropout_p (float, optional): probability of dropout\n",
    "    Inputs: inputs\n",
    "        inputs (batch, time, dim): Tensor contains input sequences\n",
    "    Outputs: outputs\n",
    "        outputs (batch, time, dim): Tensor produces by squeezeformer convolution module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        kernel_size: int = 31,\n",
    "        expansion_factor: int = 2,\n",
    "        dropout_p: float = 0.1,\n",
    "    ) -> None:\n",
    "        super(ConvModule, self).__init__()\n",
    "        assert (kernel_size - 1) % 2 == 0, \"kernel_size should be a odd number for 'SAME' padding\"\n",
    "        assert expansion_factor == 2, \"Currently, Only Supports expansion_factor 2\"\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            Transpose(shape=(1, 2)),\n",
    "            PointwiseConv1d(in_channels, in_channels * expansion_factor, stride=1, padding=0, bias=True),\n",
    "            GLU(dim=1),\n",
    "            DepthwiseConv1d(in_channels, in_channels, kernel_size, stride=1, padding=(kernel_size - 1) // 2),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            Swish(),\n",
    "            PointwiseConv1d(in_channels, in_channels, stride=1, padding=0, bias=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.sequential(inputs).transpose(1, 2)\n",
    "\n",
    "\n",
    "class TimeReductionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 1,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 2,\n",
    "    ) -> None:\n",
    "        super(TimeReductionLayer, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            DepthwiseConv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            Swish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        outputs = self.sequential(inputs.unsqueeze(1))\n",
    "        batch_size, channels, subsampled_lengths, subsampled_dim = outputs.size()\n",
    "\n",
    "        outputs = outputs.permute(0, 2, 1, 3)\n",
    "        outputs = outputs.contiguous().view(batch_size, subsampled_lengths, channels * subsampled_dim)\n",
    "\n",
    "        output_lengths = input_lengths >> 1\n",
    "        output_lengths -= 1\n",
    "        return outputs, output_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from squeezeformer.attention import MultiHeadedSelfAttentionModule\n",
    "from squeezeformer.convolution import ConvModule, DepthwiseConv2dSubsampling, TimeReductionLayer\n",
    "from squeezeformer.modules import FeedForwardModule, ResidualConnectionModule, recover_resolution\n",
    "\n",
    "\n",
    "class SqueezeformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeezeformer encoder first processes the input with a convolution subsampling layer and then\n",
    "    with a number of squeezeformer blocks.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int, optional): Dimension of input vector\n",
    "        encoder_dim (int, optional): Dimension of squeezeformer encoder\n",
    "        num_layers (int, optional): Number of squeezeformer blocks\n",
    "        reduce_layer_index (int, optional): The layer index to reduce sequence length\n",
    "        recover_layer_index (int, optional): The layer index to recover sequence length\n",
    "        num_attention_heads (int, optional): Number of attention heads\n",
    "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
    "        conv_expansion_factor (int, optional): Expansion factor of squeezeformer convolution module\n",
    "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
    "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
    "        conv_dropout_p (float, optional): Probability of squeezeformer convolution module dropout\n",
    "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
    "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
    "    Inputs: inputs, input_lengths\n",
    "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "        - **input_lengths** (batch): list of sequence input lengths\n",
    "    Returns: outputs, output_lengths\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by squeezeformer encoder.\n",
    "        - **output_lengths** (batch): list of sequence output lengths\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 80,\n",
    "        encoder_dim: int = 512,\n",
    "        num_layers: int = 16,\n",
    "        reduce_layer_index: int = 7,\n",
    "        recover_layer_index: int = 15,\n",
    "        num_attention_heads: int = 8,\n",
    "        feed_forward_expansion_factor: int = 4,\n",
    "        conv_expansion_factor: int = 2,\n",
    "        input_dropout_p: float = 0.1,\n",
    "        feed_forward_dropout_p: float = 0.1,\n",
    "        attention_dropout_p: float = 0.1,\n",
    "        conv_dropout_p: float = 0.1,\n",
    "        conv_kernel_size: int = 31,\n",
    "        half_step_residual: bool = False,\n",
    "    ):\n",
    "        super(SqueezeformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.reduce_layer_index = reduce_layer_index\n",
    "        self.recover_layer_index = recover_layer_index\n",
    "        self.conv_subsample = DepthwiseConv2dSubsampling(in_channels=1, out_channels=encoder_dim)\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(encoder_dim * (((input_dim - 1) // 2 - 1) // 2), encoder_dim),\n",
    "            nn.Dropout(p=input_dropout_p),\n",
    "        )\n",
    "        self.time_reduction_layer = TimeReductionLayer()\n",
    "        self.time_reduction_proj = nn.Linear((encoder_dim - 1) // 2, encoder_dim)\n",
    "        self.time_recover_layer = nn.Linear(encoder_dim, encoder_dim)\n",
    "        self.recover_tensor = None\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx in range(num_layers):\n",
    "            if idx < reduce_layer_index:\n",
    "                self.layers.append(\n",
    "                    SqueezeformerBlock(\n",
    "                        encoder_dim=encoder_dim,\n",
    "                        num_attention_heads=num_attention_heads,\n",
    "                        feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "                        conv_expansion_factor=conv_expansion_factor,\n",
    "                        feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "                        attention_dropout_p=attention_dropout_p,\n",
    "                        conv_dropout_p=conv_dropout_p,\n",
    "                        conv_kernel_size=conv_kernel_size,\n",
    "                        half_step_residual=half_step_residual,\n",
    "                    )\n",
    "                )\n",
    "            elif reduce_layer_index <= idx < recover_layer_index:\n",
    "                self.layers.append(\n",
    "                    ResidualConnectionModule(\n",
    "                        module=SqueezeformerBlock(\n",
    "                            encoder_dim=encoder_dim,\n",
    "                            num_attention_heads=num_attention_heads,\n",
    "                            feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "                            conv_expansion_factor=conv_expansion_factor,\n",
    "                            feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "                            attention_dropout_p=attention_dropout_p,\n",
    "                            conv_dropout_p=conv_dropout_p,\n",
    "                            conv_kernel_size=conv_kernel_size,\n",
    "                            half_step_residual=half_step_residual,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.layers.append(\n",
    "                    SqueezeformerBlock(\n",
    "                        encoder_dim=encoder_dim,\n",
    "                        num_attention_heads=num_attention_heads,\n",
    "                        feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "                        conv_expansion_factor=conv_expansion_factor,\n",
    "                        feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "                        attention_dropout_p=attention_dropout_p,\n",
    "                        conv_dropout_p=conv_dropout_p,\n",
    "                        conv_kernel_size=conv_kernel_size,\n",
    "                        half_step_residual=half_step_residual,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"Count parameters of encoder\"\"\"\n",
    "        return sum([p.numel for p in self.parameters()])\n",
    "\n",
    "    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Forward propagate a `inputs` for  encoder training.\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "        Returns:\n",
    "            (Tensor, Tensor)\n",
    "            * outputs (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n",
    "                ``(batch, seq_length, dimension)``\n",
    "            * output_lengths (torch.LongTensor): The length of output tensor. ``(batch)``\n",
    "        \"\"\"\n",
    "        outputs, output_lengths = self.conv_subsample(inputs, input_lengths)\n",
    "        outputs = self.input_proj(outputs)\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx == self.reduce_layer_index:\n",
    "                self.recover_tensor = outputs\n",
    "                outputs, output_lengths = self.time_reduction_layer(outputs, output_lengths)\n",
    "                outputs = self.time_reduction_proj(outputs)\n",
    "\n",
    "            if idx == self.recover_layer_index:\n",
    "                outputs = recover_resolution(outputs)\n",
    "                length = outputs.size(1)\n",
    "                outputs = self.time_recover_layer(outputs)\n",
    "                outputs += self.recover_tensor[:, :length, :]\n",
    "                output_lengths *= 2\n",
    "\n",
    "            outputs = layer(outputs)\n",
    "\n",
    "        return outputs, output_lengths\n",
    "\n",
    "\n",
    "class SqueezeformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SqueezeformerBlock is a simpler block structure similar to the standard Transformer block,\n",
    "    where the MHA and convolution modules are each directly followed by a single feed forward module.\n",
    "\n",
    "    Args:\n",
    "        encoder_dim (int, optional): Dimension of squeezeformer encoder\n",
    "        num_attention_heads (int, optional): Number of attention heads\n",
    "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
    "        conv_expansion_factor (int, optional): Expansion factor of squeezeformer convolution module\n",
    "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
    "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
    "        conv_dropout_p (float, optional): Probability of squeezeformer convolution module dropout\n",
    "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
    "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "    Returns: outputs\n",
    "        - **outputs** (batch, time, dim): Tensor produces by squeezeformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim: int = 512,\n",
    "        num_attention_heads: int = 8,\n",
    "        feed_forward_expansion_factor: int = 4,\n",
    "        conv_expansion_factor: int = 2,\n",
    "        feed_forward_dropout_p: float = 0.1,\n",
    "        attention_dropout_p: float = 0.1,\n",
    "        conv_dropout_p: float = 0.1,\n",
    "        conv_kernel_size: int = 31,\n",
    "        half_step_residual: bool = False,\n",
    "    ):\n",
    "        super(SqueezeformerBlock, self).__init__()\n",
    "        if half_step_residual:\n",
    "            self.feed_forward_residual_factor = 0.5\n",
    "        else:\n",
    "            self.feed_forward_residual_factor = 1.0\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            ResidualConnectionModule(\n",
    "                module=MultiHeadedSelfAttentionModule(\n",
    "                    d_model=encoder_dim,\n",
    "                    num_heads=num_attention_heads,\n",
    "                    dropout_p=attention_dropout_p,\n",
    "                ),\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "            ResidualConnectionModule(\n",
    "                module=FeedForwardModule(\n",
    "                    encoder_dim=encoder_dim,\n",
    "                    expansion_factor=feed_forward_expansion_factor,\n",
    "                    dropout_p=feed_forward_dropout_p,\n",
    "                ),\n",
    "                module_factor=self.feed_forward_residual_factor,\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "            ResidualConnectionModule(\n",
    "                module=ConvModule(\n",
    "                    in_channels=encoder_dim,\n",
    "                    kernel_size=conv_kernel_size,\n",
    "                    expansion_factor=conv_expansion_factor,\n",
    "                    dropout_p=conv_dropout_p,\n",
    "                ),\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "            ResidualConnectionModule(\n",
    "                module=FeedForwardModule(\n",
    "                    encoder_dim=encoder_dim,\n",
    "                    expansion_factor=feed_forward_expansion_factor,\n",
    "                    dropout_p=feed_forward_dropout_p,\n",
    "                ),\n",
    "                module_factor=self.feed_forward_residual_factor,\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.sequential(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd671c2924f480483fa0e4bb1160c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a1967a2ad64ce28f5e507632a70fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca54b1d90ef466380997d4e38e079be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:23:19\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.35267, Score=0.24368, Best Col-Wise Score=0.24368\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22300f519ee4a8aad59fc79a4327e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:23:38\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.32372, Score=0.28452, Best Col-Wise Score=0.28500\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3300b84868c47bc814a9f0b4e434701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:23:57\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=0, Loss=0.31091, Score=0.31131, Best Col-Wise Score=0.31193\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:23:58\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=0, Loss=0.31505, LR=2.45075e-04\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c070f732d3a945b9ba3cb43b013338db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1cfb5e41da4df8a3689a84cb8cde9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:24:16\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.30654, Score=0.32731, Best Col-Wise Score=0.32843\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4effc18877846f8b869eeb66b516f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:24:35\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.30455, Score=0.33983, Best Col-Wise Score=0.34113\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b06c6697834badb7139264d7dddb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:24:54\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=1, Loss=0.29690, Score=0.34767, Best Col-Wise Score=0.35006\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:24:58\u001b[0m | \u001b[1mINFO ] [Train] : Epoch=1, Loss=0.26352, LR=4.85150e-04\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790f7c3276584b22aea404d9a1e74bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_103580/2445575585.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4598fe4c7164d398821079874c66264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da26a22f31c4214affc5a48ec1ca10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faf892f0afd4118aede8b1854c8ea4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21073ce8107e4a7c9e923cbdbf17bda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07de3aa4ab04a84953bccb899514867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ae793354904a93939996a4cc185895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \u001b[32m2024-10-13 04:26:04\u001b[0m | \u001b[1mINFO ] [Valid] : Epoch=-1, Loss=0.31588, Score=0.35006, Best Col-Wise Score=0.35006\u001b[0m\n",
      "[ \u001b[32m2024-10-13 04:26:05\u001b[0m | \u001b[1mINFO ] First Training Results: best_score=0.35005511632018715, best_cw_score=0.35005511632018715, best_epochs=1\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Training\n",
    "config.loss_type = config.add_loss_type\n",
    "config.epochs = config.add_epochs\n",
    "config.lr = config.add_lr\n",
    "config.first_cycle_epochs = config.add_first_cycle_epochs\n",
    "\n",
    "trained_weights = sorted(\n",
    "    config.output_path.glob(f\"model_eval*.pth\"),\n",
    "    key=lambda x: int(x.stem.split('_')[-1].replace('eval', ''))\n",
    ")\n",
    "\n",
    "trainer = Trainer(config, logger)\n",
    "best_score, best_cw_score, best_epochs = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    colwise_mode=True,\n",
    "    retrain=True,\n",
    "    retrain_weight_name=trained_weights[-1].stem,\n",
    "    retrain_best_score=best_score,\n",
    ")\n",
    "logger.info(f'Additional Training Results: best_score={best_score}, best_cw_score={best_cw_score}, best_epochs={best_epochs}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122627/247072243.py:288: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(self.config.output_path / f\"model{self.save_suffix}_best.pth\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1808a193eb4d4dd0b4625dc63abd591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inference\n",
    "pred_df = trainer.test_predict(test_loader, eval_method=\"single\")\n",
    "pred_df.write_csv(config.output_path / 'submission.csv')\n",
    "\n",
    "# PostProcess\n",
    "oof_df = pl.read_parquet(config.oof_path / 'oof.parquet')\n",
    "por = PostProcessor(config, logger)\n",
    "oof_df, sub_df = por.postprocess(oof_df, pred_df)\n",
    "logger.info(f'OOF: {oof_df.shape}, Submission: {sub_df.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
